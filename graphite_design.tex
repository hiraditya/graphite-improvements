\title{Enabling Polyhedral optimization for wide use}

\abstract{
  For the past few years graphite framework, which brought polyhedral optimizations
  to gcc, was largely unmaintained. We made importatnt designed changes and fixed bugs accumulated over
  the past. The transition from cloog to isl left dead code in multiple places, and made some optimizations
  redundant. We cleaned up that code, so graphite codebase became smaller after recent changes.
  We enabled graphite to detect wider range of loops which it can optimize and discard
  difficult-to-optimize loops in a much faster way. We have also brought demand driven optimization
  into the graphite framework, since we want it to spend extra time in loops which matters,
  in the presence of profile information. 
}

\section{Goals}
Enabling widespread usage of graphite by having one flag to the end user.
Fixing accumulated bugs.
Substantial improvements on several benchmarks with no significant regression.
Propose enabling of graphite by default under one optimization flag.
Limiting the number of computations to reduce compile time.

\section{faster scop detection for selecting relevant loop nests}
The scope of the polyhedral program analysis and manipulation is a sequence of loop
nests with constant strides and affine bounds. It includes non-perfectly nested loops
and conditionals with boolean expressions of affine inequalities.
The maximal Single-Entry Single-Exit (SESE) region of the Control Flow Graph
(CFG) that satisfies those constraints is called a Static Control Part (SCoP). \ref{Girbal, Bondhugula, Trifunovic}

Current algorithm for SCoP detection in graphite was based on dominator tree where a tree (CFG) traversal is required
for analyzing an SESE. The tree traversal is linear in the number of basic blocks and SCoP detection is linear in
number of instructions. This is reasonably fast but it utilizes a generic infrastructure of SESE. With regards to
polyhedral optimization we are only interested in subtrees with loops. So it makes more sense to utilize higher level
semantics of CFG e.g., loop tree. Since higher level abstractions contain more statments, discarding them early
makes algorithm converge faster.

The new algorithm is geared towards tree traversal on loop structure. The algorithm is linear in number of loops
which is much faster than the previous algorithm. The algorithm is based on a very simple structural property of loop
tree.

LoopTree = LoopNest
LoopNest = LoopTree; nested loop

Briefly, we start the traversal at a loop-nest and analyze it recursively for validity. Once a valid loop is
found we find a valid adjacent loop. If an adjacent loop is found then we merge both loop nests
other wise we form a SCoP and resume the algorithm from the adjacent loop nest. The data structure to represent an SESE
is an ordered pair of edges (entry, exit). Choosing a simple data structure allows us to extend a SCoP in both the
directions. With this approach, the number of instructions to be analyzed for validity reduces to a minimal set.
We start by analyzing those statements which are inside a loop, because validity of those statements is
necessary for the validity of loop. The statements outside the loop nest can be just excluded from the
SESE if they are not valid.

In the graphite framework we do not include statements outside before the first and after the last loop in an SESE.
So in that sense, SCoP detected by this function may not be maximal.

GIMPLE statements belonging to the SCoP should not contain calls to functions with
side effects (pure and const function calls are allowed) and the only memory references
that are allowed are accesses through arrays with affine subscript functions.

To make the scop detection simpler we canonicalize the loops into a closed SSA form.
The previous graphite framework detected scop by analyzing an SESE. That approach resulted in a highly recursive structure
with redundancies at several places and hence, increase in compile time. In the new framework, we try to discard irrelevant
regions as fast as we can. Essentially,
\begin{enumerate}
\item Discard functions with less than two loops: Since we are mostly intersted in optimizing loop nests
or loop which has at least one sibling.
\item Start the scop detection from a CFG node which begins from a loop header.
\item Break the SCoP at a point where we find a statement which cannot be represented by graphite.
\item 
\end{enumerate}

\section{Removing limit_scops}
The functionality limit_scop was added
as an intermediate step to discard the loops which graphite could not
handle. Removing limit_scop required handling of different cases of loops and
surrounding code.  The scop is now larger so most test cases required 'number of
scops detected' to be fixed. By increasing the size of scop we can now optimize
loops which are 'siblings' of each other. This could enable loop fusion on a
number of loops.



\section{Internal parameters}
A parameter in SCoP is defined as any operand which is defined outside. These are first order parameters.
However, there are statements which only use parameters to define another operand. We call these as internal or
derived parameters.
IP = F (scop-parameters)
Since IPs can be derived from SESE parameters, they also do not require renaming when SESE is copied for polyhedral
transformations. While copying SESE we copy internal parameters at the beginning of the SCoP.
We maintain relative ordering of the internal parameters only as their ordering w.r.t. other
statements (which do not define internal parameters) does not affect the semantics of the program.
We could chose to copy IPs outside of the SCoP as well but we don't so as to preserve the original structure of the
program in case no polyhedral transformations were applied. This also preserves the live-range
of the original program and hence the register pressure.


\section{Fixing accumulated bugs}
For the past few years graphite was not actively maintained and accumulated bugs. We also believe that
moving from cloog to isl might have exposed a few corner cases of the initial implementation of graphite.
Even the graphite-identity was not working. graphite-identity is not intended to change the functionality
of the program. So the first step was to get graphite-identity to work. Second was to remove optimizations which
were either redundant because of transition to isl, or were generating incorrect code. Also, the feedback that we
got from the community was that graphite needs to operate under one flag otherwise users might not use it effectively.
The onus of generating optimal code should be on the framework and it should decide the best code to generate
for optimal data locality. So we deprecated all the flags except -fgraphite-identity and -floop-nest-optimize.
We kept -fgraphite-identity for debugging purposes only.

\section{Limiting the number of computations done in ISL}
Newer releasese of ISL have a way to bail out when total number of computations reach a limit.
This feature is very useful because this way we can limit the overhead in compile time. This also
allows greater control over optimization when loops are not too important. We provided a flag so
that users can tune it as required.

\section{Implementing loop fusion}

\section{Enabling graphite with profile guided optimization}
Although we have tried to redesign graphite for faster compile time, it still has some overhead.
Experimental results show an average of {} overhead in compile time.
Because of that, we would like to enable this for cases when compile time is not of much concern,
e.g. benchmarking, iterative compilation with feedback.

We have also enabled graphite to spend extra time on important loops. This can be done by tuning
isl-timeout programmatically based on the hotness of the loop. We believe expensive optimizations
can have the option to be demand driven so that compiler spends time on program portions which matters
most from the performance perspective. \ref{Duesterwald}


\section{Experimental Results}

\section{Conclusion and Future Work}


\references{}

S. Girbal, N. Vasilache, C. Bastoul, A. Cohen, D. Parello, M. Sigler, and O. Temam.
Semi-automatic composition of loop transformations for deep parallelism and memory
hierarchies. Intl. J. of Parallel Programming, 34(3):261–317, June 2006. Special issue on
Microgrids


U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A practical automatic
polyhedral parallelization and locality optimization system. In ACM SIGPLAN Conf. on
Programming Languages Design and Implementation (PLDI’08), Tucson, AZ, USA, June
2008.

Trifunovic, Konrad, et al. "Graphite two years after: First lessons learned
from real-world polyhedral compilation."
GCC Research Opportunities Workshop (GROW'10). 2010.


Duesterwald, Evelyn, Rajiv Gupta, and Mary Lou Soffa. "Demand-driven computation of interprocedural data flow." Proceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages. ACM, 1995.
http://www.cs.ucr.edu/~gupta/research/Publications/Comp/popl95.pdf

