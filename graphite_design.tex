\documentclass{sigplanconf}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{Impact'16}{January 18--20, 2016, Prague, Cz}
\copyrightyear{2016}
\doi{nnnnnnn.nnnnnnn}

\title{SCoP Detection: A Fast Algorithm for Industrial Compilers}
\authorinfo{Aditya Kumar, Sebastian Pop}
\authorinfo{Samsung Austin R\&D Center}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Loop optimizations are usually described on languages like Fortran where loops
and arrays are well behaved syntactic constructs.  In contrast with Fortran, low
level languages like C or C++ do not offer the same straight forward ease in
analysis of loops, induction variables, memory references, and data dependences,
essential to all high level loop transforms.  Compilers for low level languages
like GCC and LLVM lower the statements and expressions into low level constructs
common to all imperative programming languages: loops are represented by their
control flow, i.e., jumps between basic blocks, memory references are lowered
into pointer accesses, and expressions are lowered into three address code.
Thus, high level C++ expressions containing exceptions are represented on a low
level representation by an explicit complex control flow, that was implicit in
the high level language.

From a practical point of view, optimization passes in compilers like GCC and
LLVM are implemented on this low level representation, in order to avoid
duplicating analyses and optimizations for each supported language, and in order
to abstract away from the specifics of each language: for instance, consider the
semantics variability of loop statements in Fortran, C, C++, Java, and Ada
languages, all currently compiled and optimized by the middle-end of GCC.  The
price paid for the generality of the approach is extra compilation time spent in
recognizing all the high level loop constructs from the low level
representation.  This paper describes a fast algorithm to discover, from a low
level representation, loops that can be handled and optimized by a polyhedral
compiler.

\subsection{What are the boundaries of a SCoP?}

Regions of code that can be handled in the polyhedral model are usually called
Static Control Parts \cite{Bondhugula}, abbreviated SCoPs.
Usually, SCoPs may only contain regular
control flow void of exceptions or other constructs that may provoke changes in
control flow such as condition expressions dependent on data read from memory or
side effects of function calls.  As the compiler cannot easily handle such
constructs in the polyhedral model, these statements will not be integrated in a
SCoP, causing a split of the region containing such difficult statements into
two SCoPs.

To extend the applicability of polyhedral compilation, \cite{scopExtend}
presents techniques to represent general conditions, enlarging the limits of
SCoPs to full function body.  We will not consider these SCoP extension
techniques in the current paper, as we want a fast SCoP detection suitable to be
turned on by default in usual optimization levels, like ``-O2'', ``-O3'', or
``-Ofast''.  For that, we need an algorithm that is able to quickly converge on
the largest SCoPs that can profitably be transformed by a polyhedral compiler
like ISL \cite{verdoolaege2010isl} within a reasonable amount of compilation time.
Practically, on a large number of compiled programs, we want to use close to
zero overhead for functions without loops, or non interesting loops for the
polyhedral compilation, and less than ten percent of the overall compilation
time when optimizing loops for hot paths as shown in the profile of the compiled
program.

\subsection{High level overview of SCoP detection algorithms}

Existing implementations of SCoP detection based on low level representations
are based on some form of representation of the control flow graph: the first
implementation we did for Graphite \cite{graphite} was based on the control
flow graph itself and the basic block dominators tree.  Then Polly improved on
this SCoP detection \cite{polly} by working on an abstraction of the control flow graph, the
single entry single exit (SESE) region tree, a representation that integrates
the dominators tree on the control flow graph \cite{sese}.

In order to avoid constructing the region tree, and to help the SCoP detection
algorithm to faster converge on the parts of code that matter to polyhedral
compilation, we used an abstraction of the control flow graph, the natural loops
tree \cite{dragonbook}, together with dominance information.  In terms of
compilation time, both representations come at zero cost, as they are maintained
intact and correct between GCC's middle-end optimization passes, lowering the
compilation cost for the most numerous functions compiled by GCC: functions with
no loops, or with non contiguous isolated loops that cannot be profitably
transformed in a polyhedral compilation.

\subsection{Contributions of this paper}
\begin{enumerate}
  \item We present a new algorithm for SCoP detection, that improves over the
    existing techniques of extracting SCoPs from a low level intermediate
    representation,
  \item We provide a comparative analysis of the earlier algorithm implemented
    in Graphite, the SCoP detection as implemented in LLVM-Polly, and the new
    SCoP detection that we implemented and integrated in GCC 6.0,
  \item We provide an analysis of the shape of SCoPs detected by these three
    algorithms in terms of number of SCoPs detected in benchmarks, and number of
    loops per SCoP.
  \item Finally, we present experimental results on compile time improvements.
\end{enumerate}

\newpage
\section{Comparative analysis of SCoP detection algorithms}

We first describe the SCoP detection algorithms implemented originally in
Graphite and then in Polly, and then we show how the new algorithm improves over
these earlier implementations.  The common point of all these algorithms is that
they work on a low level representation of the program.  Analysis passes are
building more synthetic representations as described in the next subsection.

\subsection{Code analysis: from low level to higher level representations}

Starting from a low level representation of the program, a compiler extracts
information about specific aspects of the program through program analysis:

\begin{itemize}
\item The control flow graph (CFG) \cite{dragonbook} is built on top of a goto
  based intermediate representation: the nodes of the CFG represent the largest
  number of statements to be executed sequentially without branches, and the
  edges of the CFG correspond to the jumps between basic blocks.

\item The basic block dominator (DOM) and post-dominator (Post-DOM) trees
  \cite{dragonbook} represent the relation between basic blocks with respect to
  the control flow properties of the program: a basic block A is said to
  dominate another basic block B when all execution paths from the beginning of
  the function have to pass through A before reaching B.  Similarly, the
  post-dominator information is obtained by inverting the direction of all edges
  in the CFG and then asking the same question with respect to the block ending
  the function: A is said to post-dominate B when all paths from the end of the
  function have to pass through A to reach B in the edge-reversed CFG.

\item The single entry single exit (SESE) regions \cite{sese} are
  obtained from the CFG and the DOM and Post-DOM trees by identifying contiguous
  sequences of basic blocks dominated and post-dominated by unique basic blocks.
  The SESE regions may contain sub regions that have the SESE property: this
  inclusion relation is represented as a tree.

\label{subsec:loop-tree}
\item Natural loops \cite{dragonbook} are detected as strongly connected
  components (SCC) \cite{tarjan} on the CFG, loop nesting and sequence are
  represented under the form of a tree: loop nodes are linked with $inner$ loop,
  and $next$ loop relations.  The function body is represented as a loop at the
  root of the loop tree, and its depth is zero.  The depth of inner loops is one
  more than their parent, and sibling loops linked through $next$ are at the
  same depth.  When the CFG contains an SCC not reducible to a natural loop, for
  example two back-edges pointing back to a same basic block, all the edges and
  nodes of the CFG involved in that SCC are marked with a flag IRREDUCIBLE\_LOOP.

  % Do we really need to document this?  Are you using this property in the rest of the paper?
  %The root node has in-degree of 0, all other loop nodes have in-degree of one.
  % -- This we can remove but we need to mention that an outer loop only points to one inner loop
  % -- where all inner loops are connected by $next$ relation.

  % Same questions here.  If you need to speak about reducibility, you have to define it:
  %A CFG is said to be reducible when ...

  % Every reducible control flow graph can be represented as a loop tree.
  % -- I wanted to mention this only to show that 'almost' all programs can be analyzed for
  % -- scop detection just by using loop-tree. Such that there would be no question on
  % -- coverage of programs w.r.t. the new algorithm.

\item Static Single Assignment (SSA) form \cite{cytron}, inserts extra scalar
  variables such that each definition is unique.  The assignments to scalar
  variables are either the result of expressions, or the result of phi nodes
  placed at control flow junctions in basic blocks that are target of two or
  more control flow edges.  Assignments from phi nodes represent all the
  possible assignments considering the control flow graph.

\item An abstraction of the SSA is that of a declarative language
  \cite{spop2007} in which there are no assignments or imperative language
  constructs.  The abstraction only represents the computation of scalar
  variables that are either the result of arithmetic expressions, or the result
  of two kinds of phi nodes: loop phi nodes have two arguments, one is defined
  outside the loop containing the phi node, and the other is defined inside the
  loop.  Loop close phi nodes have only one argument that is defined in an inner
  loop.

\item The analysis of scalar evolutions (scev) \cite{scev} starts from the
  abstraction of the SSA representation described above by recognizing the
  evolution function of scalar variables for loop phi nodes, loop close phi
  nodes, and derived scalar declarations.  Loop phi nodes are declared by an
  initial value and an expression containing a self reference, when the self
  reference appears in an addition expression together with a scalar value or an
  invariant expression in the current loop, the scev represents a recursive
  function with linear or affine evolution.  Loop close phi nodes are declared
  as the last value computed by an expression that may variate in a loop, the
  scev then represents a partial recursive function.  All other scalar
  declarations can be expressed as scevs derived from declarations of other
  recursive and partial recursive functions.

\item The analysis of the number of iterations \cite{scev} provides a scev that
  represents the number of times a loop is executed.  The number of iterations
  is computed as the scev of a close phi node, or last value, of a scev starting
  at zero and incremented by one at each iteration of the loop.  The number of
  iterations can also be computed by ISL, as we provide the scevs of all the
  variables involved in each condition that we translate.

\item Pointer analysis detects base and access functions for all memory
  locations that are accessed in the program.

\item Alias analysis disambiguates the base pointers and identifies which
  pointers may or do not access the same memory locations.

\item Data reference analysis uses the results of the scev analysis to determine
  the memory access patterns of pointers in loops.

\item The delinearization analysis \cite{delinearization1, delinearization2}
  reconstructs a high level representation of arrays under the form of Fortran
  subscripts.  The delinearization uses all the access functions of all the data
  references in a loop, a region, or a function, in order to make sure all
  memory accesses follow the exact same pattern, i.e., the same subscript
  dimensions are valid in all accesses.
\end{itemize}

As we have seen, gradually, these analyzes extract from low level constructs
higher level representations: later analyzes are based on earlier lower level
results, building up a castle out of basic bricks.  All this information
synthesized by the compiler allows the representation of diverse imperative
programming languages into a polyhedral form \cite{Girbal} containing a very
high level information of loop iteration domains, memory accesses, and static
and dynamic schedules.

To reduce compilation time, the SCoP detection algorithms start from the lowest
level analysis results that are commonly available and try to quickly discard
parts of code that either cannot be translated in the polyhedral model or that
cannot be profitably transformed.  The compiler has to quickly evaluate whether
a part of the code is amenable to translation in the polyhedral model before
spending time in computing higher level costly information.  For that reason,
information about the CFG structure, and the number of loops per function are
the first checks in the SCoP detection algorithms, followed by a linear walk
over all the statements of the region of code to ensure the absence of side
effects in statements, and to gather more costly information about the number of
iterations, the linearity of memory access functions and condition expressions.

\subsection{Former algorithm for SCoP detection in Graphite}

The first Graphite SCoP detection algorithm was implemented on a very low level
representation of CFG and DOM \cite{trifunovic}.  These representations were
too restrictive for
the scev analysis to be able to determine the loops to be considered as variant
and the loops that have to be considered as invariant, in which the scev
analysis should not analyze and instantiate further scalar variables in order to
consider them as parameters.  This resulted in a very restrictive limitation of
SCoPs that had to have one full loop fully contained in the SCoP: for instance a
sequence of two loops with no surrounding loop would not be represented as a
SCoP and the SCoP detection would split these two loops into two distinct SCoPs.
This limitation has been removed in GCC 6.0 by the improvements to the scop
detection described later in this paper.

\subsection{Polly SCoP detection: based on SESE region tree}

Polly's scop detection is based on an analysis of SESE regions.  The discovery
of all the regions in a function may be expensive, specially when the number of
basic blocks in its CFG is very large.  In the current LLVM implementation of
SESE region discovery, the use of dominance frontiers may have quadratic
behavior in some cases.  The algorithm described in this paper may help in
reducing the cost of the SESE region analysis by replacing the use of dominance
frontiers with a simpler check based on the natural loops properties of depth
levels (see Figure-\ref{fig:merge-sese}) and regions of the CFG corresponding to
irreducible loops, as we will see in the next section.

\subsection{Maximal SCoPs and why they might not be very useful}
\label{subsec:maximality}
A maximal SCoP is the largest region satisfying all the properties of a SCoP and
which cannot be further extended.  This concept is currently used in Polly
\cite{polly} and was used in the previous SCoP detection of graphite
\cite{graphite}.

Since the main focus in the polyhedral model is the transformation of loops, all
code translated in the polyhedral model that is not part of a loop, or part of a
sequence of loops, only constitues an overhead in compilation time.  Finding and
representing a maximal SCoP is not necessary as it is possible to analyze all
the surrounding conditions and extract constraints on the parameters.

To illustrate our point, consider the maximal SCoP in
Figure-\ref{fig:maximality}: suppose that the region from IfCondition to EndIf
satisfies the properties of a SCoP.  It only contains a single loop, Loop1.  All
the other basic blocks IfCondition, TrueRegion, FalseRegion, and EndIf add extra
overhead to the polyhedral compilation without real benefit in terms of adding
opportunities for loop optimizations.  Having a SCoP only containing Loop1 would
prove more beneficial in terms of optimizations and overall compilation time.

\begin{figure}
\centering
%rankdir=LR;
\digraph[scale=0.5]{abc}{
BeforeSESE -> IfCondition
IfCondition -> TrueRegion
IfCondition -> FalseRegion
FalseRegion ->Loop1
Loop1 -> EndIf
TrueRegion -> EndIf
FalseRegion -> EndIf
EndIf -> AfterSESE
}
\caption{An SESE bounded by IfCondition and EndIf}
\label{fig:maximality}
\end{figure}

\section{A new faster SCoP detection to select relevant loop nests}
The new algorithm for SCoP detection works by induction on the structure of the
natural loops tree.  Section-\ref{subsec:loop-tree} has briefly introduced
the notion and properties of natural loops used in this paper; we refer our
readers to \cite{dragonbook} for an in-depth description of natural loops.

\begin{figure}
\begin{verbatim}
build_scop_depth (s1, loop):
  s1 = build_scop_depth (s1, loop->inner)
  s2 = merge_sese (s1, get_sese (loop))
  if (s2 is an invalid scop)
    {
      // s1 might be a valid scop, so return it
      // and start analyzing from the adjacent loop.
      build_scop_depth (invalid_sese, loop->next)
      return s1
    }
  if (loop is an invalid scop in s2)
    return build_scop_depth (invalid_sese, loop->next)
  return build_scop_breadth (s2, loop)

build_scop_breadth (s1, loop):
  sese_l s2 = build_scop_depth (invalid_sese, loop->next)
  if (s2 is an invalid scop)
    {
      if (s1 is a valid scop)
        add_scop (s1)
      return s1
    }
  combined = merge_sese (s1, s2)
  if (combined is a valid scop)
    s1 = combined
  else
    add_scop (s2)
  if (s1 is a valid scop)
    add_scop (s1)
  return s1
\end{verbatim}
\caption{Induction on the natural loop tree}
\label{fig:induction}
\end{figure}

\begin{figure}
\begin{verbatim}
merge_sese (first, second):
  dom = ncd (first.entry, second.entry)
  pdom = ncpd (first.exit, second.exit)
  entry = get_nearest_dom_with_single_entry (dom)
  if (!entry)
    return invalid_sese
  exit = get_nearest_pdom_with_single_exit (pdom)
  if (!exit)
    return invalid_sese;

  // Entry and exit should be in the same loop.
  // Otherwise, the region may contain edges
  // entering or leaving the region, violating
  // the Single Entry Single Exit property.
  if (loop_depth (entry->src->loop_father) !=
      loop_depth (exit->dest->loop_father))
    return invalid_sese;

  combined = new_sese (entry, exit)
  if (entry does not dominate exit
      or exit does not post-dominate entry
      or combined is an invalid scop)
    return invalid_sese
  return combined

ncd: the nearest common dominator
ncpd: the nearest common post-dominator
get_nearest_dom_with_single_entry (bb):
  if (bb has 2 predecessors e1 and e2)
    {
      if (e1.src dominates e2.src)
        return e1;
      if (e2.src dominates e1.src)
        return e2;
    }
  while (number of predecessors of bb != 1)
    bb = get_immediate_dominator (bb)
  return single predecessor edge

get_nearest_pdom_with_single_exit (bb):
  if (bb has 2 successors e1 and e2)
    {
      if (e1.dest post dominates e2.dest)
        return e1;
      if (e2.dest post dominates e1.dest)
        return e2;
    }
  while (number of successors of bb != 1)
    bb = get_immediate_post_dominator (bb)
  return single successors edge

\end{verbatim}
\caption{Merging two SESE regions}
\label{fig:merge-sese}
\end{figure}

The traversal of the natural loops tree starts at a loop-nest at depth one.  The
code inside the loop and all its nested loops are analyzed recursively for
validity, as described in Section-\ref{subsec:validity} and
Figure-\ref{fig:induction}.  Once all the validity constraints are satisfied,
the loop-nest becomes a valid scop.  After a valid loop is found, the algorithm
analyzes the next loop, a loop at same depth and immediate sibling of the loop
just analyzed.  If an adjacent loop is found to be a valid SCoP, we try to merge
both loop nests as described in Section-\ref{subsec:merge-sese} and
Figure-\ref{fig:merge-sese} by analyzing the combined region for validity.  If
the combined region represents a valid SCoP, then the combined SESE is saved and
further analysis is continued to extend the SCoP again.  If the combined SESE is
not a valid SCoP then the first SCoP is saved and analsis starts by trying to
extend the second SCoP.

While adding a new SCoP to the set of detected SCoPs, we first remove any SCoP
subsumed by the new one. This way, the SCoPs detected are mutually exclusive
w.r.t. the regions they span i.e., no SCoP interset with another in the set of
detected SCoPs.

With the new approach it is faster to discard invalid loop-nests very early. The
algorithm analyzes statements which matter most by starting the scop detection
from a loop-tree node (CFG node which begins from a loop header). This allows
discarding unrepresentable loops early in the scop-detection process, thereby
discarding SESE surrounding them.  This way, the number of instructions to be
analyzed for validity reduces to a minimal set.  We start by analyzing those
statements which are inside a loop, because validity of those statements is
necessary for the validity of loop. The statements outside the loop nest can be
just excluded from the SESE if they are not valid. Since this algorithm starts
from the loop header, it would exclude statements before the first, and after
the last loop in an SESE. Also, regions without loops maybe excluded if they are
not surrounded by loops. In that sense, SCoPs thus detected, may not be maximal
(Section-\ref{subsec:maximality}).

The optimizations enabled in polyhedral compiler ISL mostly work on loop
nests (e.g., blocking, interchange, tiling etc.), or on sibling loops (e.g.,
fusion), so we discard functions with less than two loops.

\subsection{Preconditions}
The alogrithm assumes following things to be true:
\begin{enumerate}
\item The loop tree represents a reducible CFG.
\item All the loops are in closed SSA form. Since GCC does not guarantee
loops to be in closed SSA form, we canonicalize each loop in closed SSA form
before starting the SCoP detection process.
\item Alias information is available.
\item Dominance information is available.
\item Mechanism to compute scalar evolution of a variable, in a region, is
  available.
\end{enumerate}

The alias analysis, dominance, and scalar evolution are already available in
GCC.  We recalculate the DOM because we canonicalize each loop before detecting
the SCoPs.  Since SCoP detection is like an Analysis pass, it preserves the DOM,
alias, and the scalar evolution of data references.

\subsection{Validity checks for a SCoP}
\label{subsec:validity}
An SESE region is regarded as a valid SCoP when it satisfies the following
conditions:
\begin{enumerate}
\item The entry basic block should have only one predecessor and the exit
  basic block should have only one successsor (i.e. an SESE).
\item The entry should dominate the exit.
\item The exit should post-dominate the entry.
\item The scalar evolution of all the operands in the new region should be affine.
\item All the statements inside the region should be reprepresentable in the
  polyhedral mode.  Only labels, pure function calls, assignments and
  comparison operations on integers are allowed.
\item It should be possible to represent induction variables (of all the loops)
  as signed integers because ISL might generate negative values in the optimized
  expressions which needs to be code-generated.
\item All the loops in the SESE should have single exits.
\item The number of iterations of the loops in SESE should be deterministic and
  should not overflow.
\item The loop nests in SESE should have at least one data reference.
\item The SCoP should have at least one data reference and no more than
  'MAX\_DATA\_REFERENCE' (a parameter which can be set by the user).
\item We also limit the number of parameters to a maximum value (set by
  programmer).
\end{enumerate}



\subsection{Merging sub-SCoPs}
\label{subsec:merge-sese}
We compose a larger SESE by merging two smaller SCoPs. In order to merge two
SESEs to form a new SESE, we find the nearest common dominator with single entry
and a nearest common post-dominator with single exit. After such entry and exit
have been found the combined region has to be analyzed for validity. Even if we
have analyzed the statements in a sub-scop we need to re-analyze them because
the scalar evolution of the data references change with the region of the
program under analysis.  If either such entry or exit could not be found the
algorithm does not combine the SESEs and keeps them as two separate SCoPs.

\subsection{Analysis of the SCoP detection algorithms}
The previous implementation of SCoP detection in Graphite and the existing
LLVM-Polly implementation are linear in the number of CFG edges, O(Edges).

The new algorithm is linear in the number of loops. If we consider how natural
loops are implemented in the intermediate level in GCC as well as LLVM, we need
at least two basic blocks for representing a loop. In that sense the complexity
of this algorithm is O(Loop Count) < O(Basic Blocks) <= O(Edges)

This helps in discarding invalid regions quickly. However, while analyzing
statements in an SESE for validity, all of the algorithms above, have to analyze
each statement.  The new algorithm helps analyze fewer statements in case of an
invalid SCoP because it focuses on the structure of the CFG first rather than
the validity of each statement.

\section{Experimental Results}
On Polybench:
\subsection{Original scop detection implementation in Graphite}
189 scops, max loops/scop 8, min loops/scop 1: 109 scops with 1 loop/scop, 316
loops in scops, 1.67 loops/scop without 1loop/scop, 80 scops contain 207 loops
(2.59 loops/scop)

\subsection{Scop detection algorithm in LLVM-Polly}
Since the pretty-printer of Polly does not print number of loops in SCoPs detected,
we had to patch the source code to get the results.
Number of scops detected: 30, max loops/scop 11, min loops/scop 2: 3 scops with
2 loops/scop, 155 loops in scops, 5.17 loops/scop


\subsection{The new scop detection in Graphite}
34 scops, max loops/scop 17, min loops/scop 2: 7 scops with 2 loops/scop, 207
loops in scops, 6.09 loops/scop

Observations:
\begin{enumerate}
  \item No regressions new/original scop detection, the new algorithm discovers
    larger scops (316 - 207 = 109).
  \item Removing limit-scops discovers larger scops i.e., from 2.59 to 6.09
    loops per SCoP.
\end{enumerate}

\section{Conclusion and Future Work}
We have shown that our new algorithm of SCoP detection is faster in terms of
time complexity and also detects larger SCoPs w.r.t. both previous Graphite and
LLVM-Polly implementations. This stems from the fact that operating on a higher
level data structure allows us to quickly discard uninteresting regions. We
could not get experimental data on the compile-time improvements during the SCoP
detection process because it was very difficult to precisely profile only the
SCoP-detection part of the polyhedral optimization framework. The current timing
infrastructure of gcc isn't precise enough to benchmark portions of passes which
are non-dominating on the overall compile time.

Currently, while merging two SCoPs, we try to find an SESE enclosing both. There
are techniques to add basic blocks and form SESE enclosing the SCoPs if no such
SESE was found. We would like to add this functionality in future. Since this
would have further impact on compile time we would like to do this only on hot
code paths by using the profile information.

Before starting the SCoP detection process we need to canonicalize loops to
closed SSA form. While this does not add any extra operations to the program, it
is certainly not desirable to modify the original program during the analysis
phase of any optimization pass. We would try to address this in future.  Also,
we would try to find ways to experimentally verify effectiveness of our
algorithm.

\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
