\title{Enabling Polyhedral optimization for wide use}

\abstract{
  For the past few years graphite framework, which brought polyhedral optimizations
  to gcc, was largely unmaintained. We made importatnt designed changes and fixed bugs accumulated over
  the past. The transition from cloog to isl left dead code in multiple places, and made some optimizations
  redundant. We cleaned up that code, so graphite codebase became smaller after recent changes.
  We enabled graphite to detect wider range of loops which it can optimize and discard
  difficult-to-optimize loops in a much faster way. We have also brought demand driven optimization
  into the graphite framework, since we want it to spend extra time in loops which matters,
  in the presence of profile information.
  Demand driven optimization is new approach to optimization: Moving from problem to solution.
}

\section{Goals}
\begin{enumerate}

  \item Algorithm for scop detection.
  \item Comparative analysis of llvm-polly, earlier graphite, current algorithm.
  \item Modelling dependencies. (Cross bb, memory reads/writes)
  \item SCoPs detected by llvm-poly vs. Graphite.
  \item Experimental results on compile time improvements/degradation.

\end{enumerate}


\section{faster scop detection for selecting relevant loop nests}
The scope of the polyhedral program analysis and manipulation is a sequence of loop
nests with constant strides and affine bounds. It includes non-perfectly nested loops
and conditionals with boolean expressions of affine inequalities.
The maximal Single-Entry Single-Exit (SESE) region of the Control Flow Graph
(CFG) that satisfies those constraints is called a Static Control Part (SCoP). \ref{Girbal, Bondhugula, Trifunovic}

Current algorithm for SCoP detection in graphite was based on dominator tree where a tree (CFG) traversal is required
for analyzing an SESE. The tree traversal is linear in the number of basic blocks and SCoP detection is linear in
number of instructions. This is reasonably fast but it utilizes a generic infrastructure of SESE. With regards to
polyhedral optimization we are only interested in subtrees with loops. So it makes more sense to utilize higher level
semantics of CFG e.g., loop tree. Since higher level abstractions contain more statments, discarding them early
makes algorithm converge faster.

The new algorithm is geared towards tree traversal on loop structure. The algorithm is linear in number of loops
which is much faster than the previous algorithm. The algorithm is based on a very simple structural property of loop
tree.

LoopTree = LoopNest
LoopNest = LoopTree; nested loop

Briefly, we start the traversal at a loop-nest and analyze it recursively for validity. Once a valid loop is
found we find a valid adjacent loop. If an adjacent loop is found then we merge both loop nests
other wise we form a SCoP and resume the algorithm from the adjacent loop nest. The data structure to represent an SESE
is an ordered pair of edges (entry, exit). Choosing a simple data structure allows us to extend a SCoP in both the
directions. With this approach, the number of instructions to be analyzed for validity reduces to a minimal set.
We start by analyzing those statements which are inside a loop, because validity of those statements is
necessary for the validity of loop. The statements outside the loop nest can be just excluded from the
SESE if they are not valid.

In the graphite framework we do not include statements outside before the first and after the last loop in an SESE.
So in that sense, SCoP detected by this function may not be maximal.

GIMPLE statements belonging to the SCoP should not contain calls to functions with
side effects (pure and const function calls are allowed) and the only memory references
that are allowed are accesses through arrays with affine subscript functions.

To make the scop detection simpler we canonicalize the loops into a closed SSA form.
The previous graphite framework detected scop by analyzing an SESE. That approach resulted in a highly recursive structure
with redundancies at several places and hence, increase in compile time. In the new framework, we try to discard irrelevant
regions as fast as we can. Essentially,
\begin{enumerate}
\item Discard functions with less than two loops: Since we are mostly intersted in optimizing loop nests
or loop which has at least one sibling.
\item Start the scop detection from a CFG node which begins from a loop header.
\item Break the SCoP at a point where we find a statement which cannot be represented by graphite.
\item 
\end{enumerate}

\section{Removing limit_scops}
The functionality limit_scop was added
as an intermediate step to discard the loops which graphite could not
handle. Removing limit_scop required handling of different cases of loops and
surrounding code.  The scop is now larger so most test cases required 'number of
scops detected' to be fixed. By increasing the size of scop we can now optimize
loops which are 'siblings' of each other. This could enable loop fusion on a
number of loops.



\section{Internal parameters, derived parameters}
A parameter in SCoP is defined as any operand which is defined outside. These are first order parameters.
However, there are statements which only use parameters to define another operand. We call these as internal or
derived parameters.
IP = F (scop-parameters)
Since IPs can be derived from SESE parameters, they also do not require renaming when SESE is copied for polyhedral
transformations. While copying SESE we copy internal parameters at the beginning of the SCoP.
We maintain relative ordering of the internal parameters only as their ordering w.r.t. other
statements (which do not define internal parameters) does not affect the semantics of the program.
We could chose to copy IPs outside of the SCoP as well but we don't so as to preserve the original structure of the
program in case no polyhedral transformations were applied. This also preserves the live-range
of the original program and hence the register pressure.


\section{Modelling dependencies}

The execution order is modelled...
The data flow dependencies are modelled on a case by case basis.

Memory accesses are modelled as affine sequence of memory accesses.
Scalar references which are local to a basic block are ...
Scalar references which cross the basic block can be of three kinds.
\begin{enumerate}
  \item Cross basic block scalar dependencies within a SCoP: We create an edge from the producer to the consumer.
    PHIs are also cross-bb dependencies and we preserve them.
  \item Reads from outside of SCoPs: These references are modelled as parameter to the SCoP.
  \item Writes to outside of SCoP: These references require a closed PHI node at the end of the SCoP.
\end{enumerate}

Unlike Polly which treats each basic blocks as independent by removing cross basic block dependencies, we preserve
the dependencies. Our approach prevents redundant code to be generated and preserves the original program
if no optimization took place. This has several advantages:
\begin{enumerate}
  \item No cleanup required and hence we have more flexibility on where to invoke graphite in the pass manager.
  \item This also allows us to generate the optimized code in SSA form which we do. Preserving SSA form on the optimized
    code enables other high level optimizations.
\end{enumerate}


\section{merging sub-scops to form larger scops}
We compose larger SCoPs by merging smaller ones at the same loop depth. After combining the sub-scops
the newly formed scop has to be reanalyzed for all the validity requirements:
\begin{enumerate}
  \item The entry basic block should have only one predecessor and the exit basic block should have only one
    successsor.
  \item The entry should dominate the exit.
  \item The exit should post-dominate the entry.
  \item Writes to outside of SCoP.
  \item The scalar evolution of all the operands in the new region should be affine.

    \begin{enumerate}
      \item The statement should not have any side effects.
      \item Only label, pure call, assignments and comparison operations on integers are allowed.
    \end{enumerate}
\end{enumerate}

Even if we have analyzed the statements in a sub-scop we need to redo the whole thing because the scalar evolution
of the references change with the scope of the program under analysis.

\section{Maximal SCoPs and why they might not be very useful.}
A Maximal SCoP is the larges such SCoP which cannot be extended further. This concept is used
in LLVM \ref{Tobi} and in previous graphite implementation \ref{Graphite}. Since the main focus of
polyhedral optimizations is the loops and code surrounding it, finding a maximal SCoP might introduce branches
which are not related to loops at all. e.g.,

\begin{comment}
Scop
Cond
|
|--True Region
|
|--False Region
|       | Loop1
|       |
\end{comment}

In this example The cond and True regions just add extra code to the polyhedral analysis and transformations if there
are no loops in it. Even if there are loops in True region they better be analyzed separately because
execution of Loop1 and True Region are mutually exclusive.
Having just the Loop1 as SCoP would prove more beneficial in terms of optimizations and overall compile time.


\section{Region based SCoP detection of LLVM}


\section{Experimental Results}

\section{Conclusion and Future Work}


\references{}

S. Girbal, N. Vasilache, C. Bastoul, A. Cohen, D. Parello, M. Sigler, and O. Temam.
Semi-automatic composition of loop transformations for deep parallelism and memory
hierarchies. Intl. J. of Parallel Programming, 34(3):261–317, June 2006. Special issue on
Microgrids


U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A practical automatic
polyhedral parallelization and locality optimization system. In ACM SIGPLAN Conf. on
Programming Languages Design and Implementation (PLDI’08), Tucson, AZ, USA, June
2008.

Trifunovic, Konrad, et al. "Graphite two years after: First lessons learned
from real-world polyhedral compilation."
GCC Research Opportunities Workshop (GROW'10). 2010.


Duesterwald, Evelyn, Rajiv Gupta, and Mary Lou Soffa. "Demand-driven computation of interprocedural data flow." Proceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages. ACM, 1995.
http://www.cs.ucr.edu/~gupta/research/Publications/Comp/popl95.pdf

