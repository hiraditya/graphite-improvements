\documentclass{sig-alternate}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}

\begin{document}

\def \SCoP {SCoP}
\def \GCC {GCC}
\def \LLVM {LLVM}
\def \SESE {SESE}
\def \CFG {CFG}
\def \scev {scev}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\title{\SCoP{} Detection: A Fast Algorithm for Industrial Compilers}

\toappear{
   \hrule \vspace{5pt}
   IMPACT 2016\\
   Sixth International Workshop on Polyhedral Compilation Techniques\\
   Jan 19, 2016, Prague, Czech Republic\\
   In conjunction with HiPEAC 2016.\\[10pt]
   \url{http://impact.gforge.inria.fr/impact2016}\\
}
\numberofauthors{2}

\author{
\alignauthor
Aditya Kumar\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{aditya.k7@samsung.com}
\alignauthor
Sebastian Pop\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{s.pop@samsung.com}
}

\maketitle

\begin{abstract}
\SCoP{} detection is a search algorithm that a compiler for imperative programming
languages is using to find loops to be represented and optimized in the
polyhedral model.

We improved the current algorithms for \SCoP{} detection by operating on the
natural loops tree, a higher level representation of the \CFG{,} in order to lower
the overall compilation time.  The algorithm described in this paper has been
implemented in \GCC{} 6.0 as a requirement to enable by default the isl schedule
optimizer at ``-O3 -fprofile-use''.

We present evidence that the new \SCoP{} detection algorithm improves the overall
compilation time: on a large C++ application, the overall compilation time spent
in \SCoP{} detection was reduced from $7\%$ to $0.3\%$.  Experimental results also
show that \GCC{} detects larger \SCoP{s} on Polybench: $6.09$ loops per \SCoP{} as
compared to $2.59$ loops per \SCoP{} with the previous algorithm.
\end{abstract}

\section{Introduction}

Loop optimizations are usually described for languages like Fortran where loops
and arrays are well-behaved syntactic constructs.  In contrast with Fortran,
low-level languages like C or C++ do not offer similar ease in the analysis of
loops, induction variables, memory references, and data dependences, which are
essential to all high-level loop transforms.  Compilers for low-level languages
like \GCC{} and \LLVM{} lower the statements and expressions into low-level constructs
common to all imperative programming languages: loops are represented by their
control flow, i.e., jumps between basic blocks, memory references are lowered
into pointer accesses, and expressions are lowered into three-address code.

From a practical point of view, optimization passes in compilers like \GCC{} and
\LLVM{} are implemented on this low-level intermediate representation (IR) in order
to avoid duplicating analyses and optimizations for each supported language, and
in order to abstract away from the specifics of each language: for instance,
consider the semantics variability of loop statements in Fortran, C, C++, Java,
and Ada languages, all currently compiled and optimized by the middle end of
\GCC{.}  The price paid for the generality of the approach is extra compilation
time spent in recognizing all the high-level loop constructs from the low-level
representation.  This paper describes a fast algorithm to discover, from a
low-level representation, loops that can be handled and optimized by a
polyhedral compiler.

\subsection{What are the boundaries of a \SCoP{?}}

Regions of code that can be handled in the polyhedral model are usually called
Static Control Parts \cite{Girbal,Bondhugula}, abbreviated as \SCoP{s}.  Usually,
\SCoP{s} may only contain regular control flow free of exceptions and other
constructs that may provoke changes in control flow such as conditional
expressions dependent on data (read from memory) or side effects of function
calls.  As the compiler cannot easily handle such constructs in the polyhedral
model these statements are not integrated in a \SCoP{,} causing a split of the
region containing such difficult statements into two \SCoP{s}.

To extend the applicability of polyhedral compilation, \cite{scopExtend}
presents techniques to represent general conditions, enlarging the limits of
\SCoP{s} to full function bodies.  We will not consider these \SCoP{} extension
techniques in the current paper, as we want a fast \SCoP{} detection suitable to be
turned on by default at usual optimization levels, like ``-O2'', ``-O3'', and
``-Ofast''.  For that, we need an algorithm that is able to quickly converge on
the largest \SCoP{s} that can profitably be transformed by a polyhedral compiler
like isl \cite{verdoolaege2010isl} within a reasonable amount of compilation
time.  Practically, on a large number of compiled programs, we want close to
zero overhead for functions without loops and for functions without any
interesting loops to polyhedral compilation, and less than ten percent of the
overall compilation time when optimizing loops for paths shown as hot in the
profile of the compiled program.

\subsection{High level overview of \SCoP{} detection algorithms}

Existing implementations of \SCoP{} detection based on low-level representations
are based on some form of representation of the control flow graph: the first
implementation we did for Graphite \cite{graphite} was based on the control flow
graph itself and the basic-block dominators tree.  Then Polly improved on this
\SCoP{} detection \cite{polly, grosser2012polly} by working on an abstraction of
the control-flow graph: the Single Entry Single Exit (\SESE{}) regions tree, a
representation that integrates the dominator tree with the control flow graph
\cite{sese}.

In order to avoid constructing the regions tree, and to help the \SCoP{}
detection algorithm converge faster on the parts of code that matter to
polyhedral compilation, we use an abstraction of the control flow graph -- the
natural loops tree \cite{dragonbook} -- together with dominance information
\cite{ramalingam}.  In terms of compilation time, both representations come at
zero cost, as they are maintained intact and correct between \GCC{'}s middle-end
optimization passes, lowering the compilation cost for the most numerous
functions compiled by \GCC{:} functions with no loops, or with non-contiguous
isolated loops that cannot be profitably transformed in a polyhedral
compilation.

\subsection{Contributions of this paper}
\begin{enumerate}
  \item We present a new algorithm for \SCoP{} detection that improves the
    existing techniques of extracting \SCoP{s} from a low-level intermediate
    representation.
  \item We provide a comparative analysis of the earlier algorithm implemented
    in Graphite, the \SCoP{} detection as implemented in \LLVM{-}Polly, and the new
    \SCoP{} detection that we implemented and integrated into \GCC{.}
  \item We provide an analysis of the shape of \SCoP{s} detected by these three
    algorithms in terms of the number of \SCoP{s} detected in benchmarks and the
    number of loops per \SCoP{.}
  \item We present results on compilation time improvements.
\end{enumerate}

\section{Comparative analysis of \SCoP{} detection algorithms}

We first describe the \SCoP{} detection algorithms implemented originally in
Graphite and in Polly, and then we show how the new algorithm improves over
these earlier implementations.  The common point of all these algorithms is that
they work on a low-level representation of the program.  Analysis passes are
building more synthetic representations as described in the next subsection.

\subsection{Code analysis: from low-level to higher-level representations}

Starting from a low-level representation of the program, a compiler extracts
information about specific aspects of the program through program analysis:

\begin{itemize}
\item The control flow graph (\CFG{}) \cite{dragonbook} is built on top of a goto
  based intermediate representation: each node of the \CFG{} represents a basic
  block i.e., the largest number of statements to be executed sequentially
  without branches, and the edges of the \CFG{} correspond to the jumps between
  basic blocks.

\item The basic-block dominator (DOM) and post-dominator (Post-DOM) trees
  \cite{dragonbook, ramalingam} represent the relations between basic blocks with
  respect to the control flow properties of the program: a basic block A is said
  to dominate another basic block B when all execution paths from the beginning
  of the function have to pass through A before reaching B.  Similarly, the
  post-dominator information is obtained by inverting the direction of all edges
  in the \CFG{} and then asking the same question with respect to the block ending
  the function: A is said to post-dominate B when all paths from the end of the
  function have to pass through A to reach B in the edge-reversed \CFG{.}

\item The Single Entry Single Exit (\SESE{}) regions \cite{sese} are obtained from
  the \CFG{} and the DOM and Post-DOM trees by identifying contiguous sequences of
  basic blocks dominated and post-dominated by unique basic blocks.  In \LLVM{} the
  computation of \SESE{} regions tree is based on iterated dominance frontier
  \cite{ramalingam} that can be quadratic in some cases.  The \SESE{} regions may
  contain sub regions that have the \SESE{} property: this inclusion relation is
  represented as a tree.

\label{subsec:loop-tree}
\item Natural loops \cite{dragonbook, ramalingam} are detected as strongly connected
  components (SCC) \cite{tarjan} on the \CFG{,} loop nesting and sequence are
  represented under the form of a tree: loop nodes are linked with $inner$ loop
  and $next$ loop relations.  The function body is represented as a loop at the
  root of the loop tree, and its depth is zero.  The depth of inner loops is one
  more than their parent, and sibling loops linked through $next$ are at the
  same depth.  When the \CFG{} contains an SCC that is not reducible to a natural
  loop, for example two back-edges pointing back to a same basic block, all the
  edges and nodes of the \CFG{} involved in that SCC are marked with a flag
  IRREDUCIBLE\_LOOP.

\item Static Single Assignment (SSA) form \cite{cytron} inserts extra scalar
  variables such that each definition is unique.  The assignments to scalar
  variables are either the result of expressions, or the result of phi nodes
  placed at control flow junctions in basic blocks that are target of two or
  more control flow edges.  Assignments from phi nodes represent all the
  possible assignments considering the control flow graph. The abstraction of
  SSA representation is that of a declarative language \cite{spop2007} in which
  there are no assignments or imperative language constructs.  The abstraction
  only represents the computation of scalar variables that are either the result
  of arithmetic expressions, or the result of two kinds of phi nodes loop phi
  nodes and loop close phi nodes. Loop phi nodes have two arguments, one is
  defined outside the loop containing the phi node, and the other is defined
  inside the loop.  Loop close phi nodes have only one argument that is defined
  in an inner loop.

\item The analysis of scalar evolutions (\scev{}) \cite{scev} starts from the
  abstraction of the SSA representation described above by recognizing the
  evolution function of scalar variables for loop phi nodes, loop close phi
  nodes, and derived scalar declarations.  Loop phi nodes are declared by an
  initial value and an expression containing a self reference, when the self
  reference appears in an addition expression together with a scalar value or an
  invariant expression in the current loop, the \scev{} represents a recursive
  function with linear or affine evolution.  Loop close phi nodes are declared
  as the last value computed by an expression that may variate in a loop, the
  \scev{} then represents a partial recursive function.  All other scalar
  declarations can be expressed as \scev{s} derived from declarations of other
  recursive and partial recursive functions.

\item The analysis of the number of iterations \cite{scev} provides a \scev{} that
  represents the number of times a loop is executed.  The number of iterations
  is computed as the \scev{} of a close phi node, or last value, of a \scev{} starting
  at zero and incremented by one at each iteration of the loop.  The number of
  iterations can also be computed by isl, as we provide the \scev{s} of all the
  variables involved in each condition that we translate.

\item Pointer analysis detects base and access functions for all memory
  locations that are accessed in the program.

\item Alias analysis disambiguates the base pointers and identifies which
  pointers may access the same memory locations.

\item Data reference analysis uses the results of the \scev{} analysis to determine
  the memory access patterns of pointers in loops.

\item The delinearization analysis \cite{delinearization1, delinearization2}
  reconstructs a high-level representation of arrays under the form of Fortran
  subscripts.  The delinearization uses all the access functions of all the data
  references in a loop, a region, or a function, in order to make sure all
  memory accesses follow the exact same pattern, i.e., the same subscript
  dimensions are valid in all accesses.
\end{itemize}

As we have seen, gradually, these analyzes extract from low-level constructs
higher-level representations: later analyzes are based on earlier lower-level
results, building up a castle out of basic bricks.  All this information
synthesized by the compiler allows the representation of diverse imperative
programming languages into a polyhedral form \cite{Girbal} containing a very
high-level information of loop iteration domains, memory accesses, and static
and dynamic schedules.

To reduce compilation time, the \SCoP{} detection algorithms start from the
lowest-level analysis results that are commonly available and try to quickly
discard parts of code that either cannot be translated in the polyhedral model
or that cannot be profitably transformed.  The compiler has to quickly evaluate
whether a part of the code is amenable to translation in the polyhedral model
before spending time in computing higher-level costly information.  For that
reason, information about the \CFG{} structure, and the number of loops per
function are the first checks in the \SCoP{} detection algorithms, followed by a
linear walk over all the statements of the region of code to gather more costly
information.

\subsection{Former \SCoP{} detection in Graphite}
\label{subsec:graphite-SCoP}
The first Graphite \SCoP{} detection algorithm was implemented on a very low-level
representation of \CFG{} and DOM \cite{graphite}.  These representations were too
restrictive for the \scev{} analysis to be able to determine the loops to be
considered as variant and the loops that have to be considered as invariant, in
which the \scev{} analysis should not analyze and instantiate further scalar
variables in order to consider them as parameters \cite{scev}.  This resulted in
a very restrictive limitation of \SCoP{s} that had to have one full loop fully
contained in the \SCoP{:} for instance a sequence of two loops with no surrounding
loop would not be represented as a \SCoP{} and the \SCoP{} detection would split these
two loops into two distinct \SCoP{s}.  This limitation has been removed in \GCC{} 6.0
by the improvements to the \SCoP{} detection described later in this paper.

\subsection{Polly \SCoP{} detection on \SESE{} regions}
\label{subsec:polly-SCoP}
Polly's \SCoP{} detection is based on an analysis of \SESE{} regions \cite{polly}.
The discovery
of all the regions in a function may be expensive, especially when the number of
basic blocks in its \CFG{} is very large.  In the current \LLVM{} implementation of
\SESE{} region discovery, the use of dominance frontiers may have quadratic
behavior in some cases.  The algorithm described in this paper may help in
reducing the cost of the \SESE{} region analysis by replacing the use of dominance
frontiers with a simpler check based on the natural loops properties of depth
levels and regions of the \CFG{} corresponding to irreducible loops, see
Figure-\ref{fig:merge-sese} in Section-\ref{sec:new-SCoP-detection}.

\subsection{Why maximal \SCoP{s} are not very useful}
\label{subsec:maximality}
A maximal \SCoP{} is the largest region satisfying all the properties of a \SCoP{} and
which cannot be further extended.  This concept is currently used in Polly
\cite{polly} and was used in the previous \SCoP{} detection of Graphite
\cite{graphite}.

Since the main focus in the polyhedral model is the transformation of loops, all
code translated in the polyhedral model that is not part of a loop, or part of a
sequence of loops, only constitutes an overhead in compilation time.  Finding and
representing a maximal \SCoP{} is not necessary as it is possible to analyze all
the surrounding conditions and extract constraints on the parameters.

To illustrate our point, consider the maximal \SCoP{} in
Figure-\ref{fig:maximality}: suppose that the region from IfCondition to EndIf
satisfies the properties of a \SCoP{,} and contains a single loop composed of a
LoopHeader and a LoopLatch.  All the other blocks of a maximal \SCoP{}
IfCondition, TrueRegion, and EndIf add extra overhead to the polyhedral
compilation without real benefit in terms of adding opportunities for loop
optimizations.  Having a \SCoP{} only containing the loop would prove more
beneficial in terms of optimizations and overall compilation time.

The greedy approach of detecting maximal \SCoP{s} is not necessarily optimal in
terms of compilation time, as well as in terms of performance achieved:
detecting smaller \SCoP{s} may prove beneficial to compilation time, as well as
narrowing the choices of possible optimizations that would again lead to
compilation time improvements.

\begin{figure}
\centering
%rankdir=LR;
\digraph[scale=0.35]{abc}{
BeforeSESE -> IfCondition
IfCondition -> TrueRegion
IfCondition -> LoopHeader
LoopHeader -> LoopLatch
LoopLatch -> LoopHeader
LoopHeader -> EndIf
TrueRegion -> EndIf
EndIf -> AfterSESE
}
\caption{Maximal \SCoP{} bounded by IfCondition and EndIf}
\label{fig:maximality}
\end{figure}

\section{A new faster \SCoP{} detection}
\label{sec:new-SCoP-detection}
The new algorithm for \SCoP{} detection works by induction on the structure of the
natural loops tree as described in Section-\ref{subsec:induction} and listed in
Figure-\ref{fig:induction}.  The traversal of the loop tree tries to enlarge a
region by attaching adjacent or outer valid regions as described in
Section-\ref{subsec:merge-sese} and listed in Figure-\ref{fig:merge-sese}.
Section-\ref{subsec:loop-tree} has a brief introduction to the notions and
properties used in this section; we refer our readers to \cite{ramalingam} for
an in-depth discussion.  The algorithm assumes the following preconditions
during the \SCoP{} detection:
\begin{itemize}
\item The natural loops tree represents a reducible \CFG{:} all edges and basic
  blocks in an irreducible SCC are tagged with an IRREDUCIBLE\_LOOP flag.
\item Dominance information is available.
\item Mechanism to compute the evolution of scalars in a region exists.
\end{itemize}

\subsection{Induction on the structure of natural loops}
\label{subsec:induction}

\begin{figure}
\begin{verbatim}
// Recurse on the loop.inner.
sese build_scop_depth (sese s1, loop l):
  // sese: { edge, edge }
  s1 = build_scop_depth (s1, l.inner)
  sese s2 = merge_sese (s1, get_sese (l))
  if (s2 is an invalid scop)
    {
      // s1 might be a valid scop, so return it
      // and start analyzing from the adjacent loop.
      build_scop_depth (invalid_sese, l.next)
      return s1
    }
  if (l is an invalid scop in s2)
    return build_scop_depth (invalid_sese, l.next)
  return build_scop_breadth (s2, l)

// Recurse on loop.next.
sese build_scop_breadth (sese s1, loop l):
  sese s2 = build_scop_depth (invalid_sese, l.next)
  if (s2 is an invalid scop)
    {
      if (s1 is a valid scop)
        add_scop (s1)
      return s1
    }
  sese combined = merge_sese (s1, s2)
  if (combined is a valid scop)
    s1 = combined
  else
    add_scop (s2)
  if (s1 is a valid scop)
    add_scop (s1)
  return s1
\end{verbatim}
\caption{Induction on the structure of natural loops}
\label{fig:induction}
\end{figure}

The traversal of the natural loops tree starts at a loop-nest at depth one
(the loop at depth zero is the function body).
The code inside the loop and all its nested loops are analyzed recursively for
validity, as described in Section-\ref{subsec:validity} and
function \texttt{build\_scop\_depth} in
Figure-\ref{fig:induction}.  Once all the validity constraints are satisfied,
the loop-nest becomes a valid \SCoP{} and it is saved in a set of
already found \SCoP{s}. While adding a new \SCoP{} to the set of detected \SCoP{s}, we
first remove any \SCoP{} which either is a sub-\SCoP{} (completely surrounded) or
intersects (partially overlaps), with the new one. The way algorithm runs, from
bottom up for each loop nest, any new \SCoP{} to be added cannot be subsumed by an
existing \SCoP{} in the set. This way, the \SCoP{s} maintained in the set are
mutually exclusive w.r.t. the regions they span i.e., no \SCoP{} intersects with
another in the set of detected \SCoP{s}.

After a valid loop is found, the algorithm analyzes the next loop
(see \texttt{build\_scop\_breadth} in Figure-\ref{fig:induction}), a loop at
same depth and immediate sibling of the loop just analyzed.  If an adjacent loop
is found to be a valid \SCoP{,} we try to merge both loop nests as described in
Section-\ref{subsec:merge-sese} and Figure-\ref{fig:merge-sese}.  If a combined
\SESE{} has been found, which subsumes both \SCoP{s}, it is analyzed for
validity. Even if we have already analyzed the statements in the combined \SESE{}
in their respective sub-\SCoP{s}, we need to re-analyze them because the scalar
evolution of the data references change with the region of the program under
analysis. If the combined \SESE{} represents a valid \SCoP{,} then it is saved, after
removing any intersecting or sub-\SCoP{s}, and further analysis is continued to
extend the \SCoP{} again.  If no such \SESE{} could be found, the algorithm keeps them
as two separate \SCoP{s}, and continues by trying to extend the second \SCoP{.}

With the new approach it is faster to discard many invalid loop-nests early. The
algorithm analyzes statements which matter most by starting the \SCoP{} detection
from a loop-tree node (\CFG{} node which begins from a loop header). This allows
discarding unrepresentable loops early in the \SCoP{}-detection process, thereby
discarding \SESE{} region surrounding them.  This way, the number of instructions
to be analyzed for validity reduces to a minimal set.  We start by analyzing
those statements which are inside a loop, because validity of those statements
is necessary for the validity of loop.  The statements outside the loop nest can
be excluded from the \SESE{} if they are not valid.  Since this algorithm starts
from the loop header, it excludes statements before the first, and after
the last loop in an \SESE{.}  Also, regions without loops are be excluded if they
are not surrounded by loops.  \SCoP{s} thus detected are not maximal, in contrast
with the example and discussion in Section-\ref{subsec:maximality}.

\subsection{When is an \SESE{} region a valid \SCoP{?}}
\label{subsec:validity}
An \SESE{} region is regarded as a valid \SCoP{} when it satisfies the following
conditions:
\begin{enumerate}
\item The entry basic block should have only one predecessor and the exit
  basic block should have only one successor (i.e., an \SESE{}).
\item The entry should dominate the exit.
\item The exit should post-dominate the entry.
\item All the loops in the \SESE{} should have single exits.
\item The scalar evolution of all memory accesses and conditional expressions
  should be affine.
\item All the statements inside the region should be representable in the
  polyhedral model.  For example, labels, pure function calls, assignments and
  comparison operations on integer types are allowed.
\item The induction variables of all the loops should be of (or convertible to)
  signed integer type because isl might generate negative values in the optimized
  expressions which would have to be code-generated.
\end{enumerate}

\subsection{Merging \SCoP{s}}
\label{subsec:merge-sese}

\begin{figure}
\begin{verbatim}
sese_t merge_sese (sese_t first, sese_t second):
  // ncd: the nearest common dominator
  // ncpd: the nearest common post-dominator
  // sese: { edge, edge }, bb: basic block

  bb dom = ncd (first.entry, second.entry)
  bb pdom = ncpd (first.exit, second.exit)

  edge entry = nearest_dom_with_single_entry (dom)
  if (!entry)
    return invalid_sese
  edge exit = nearest_pdom_with_single_exit (pdom)
  if (!exit)
    return invalid_sese

  // entry and exit should be in the same loop.
  // and hence in the same sese.
  if (loop_depth (entry.src.loop_father) !=
      loop_depth (exit.dest.loop_father))
    return invalid_sese

  // edge should belong to reducible loop.
  if (entry.flag == EDGE_IRREDUCIBLE_LOOP
      or exit.flag == EDGE_IRREDUCIBLE_LOOP)
    return invalid_sese

  sese_t combined = new_sese (entry, exit)
  if (entry does not dominate exit
      or exit does not post-dominate entry
      or combined is an invalid scop)
    return invalid_sese
  return combined
\end{verbatim}
\caption{Merging two \SESE{} regions}
\label{fig:merge-sese}
\end{figure}

\begin{figure}
\begin{verbatim}
edge nearest_dom_with_single_entry (bb b):
  if (b has 1 predecessor edge e)
    return e
  if (b has 2 predecessor edges e1 and e2)
    { // Check for a back-edge
      if (e1.src dominates e2.src)
        return e1
      if (e2.src dominates e1.src)
        return e2
    }
  b = get_immediate_dominator (b)
  return nearest_dom_with_single_entry (b)

edge nearest_pdom_with_single_exit (bb b):
  if (b has 1 successor edge e)
    return e
  if (b has 2 successor edges e1 and e2)
    { // Check for a back-edge
      if (e1.dest post dominates e2.dest)
        return e1
      if (e2.dest post dominates e1.dest)
        return e2
    }
  b = get_immediate_post_dominator (b)
  return nearest_pdom_with_single_exit (b)
\end{verbatim}
\caption{Iterative computation of a single entry dominator and a single exit
  post-dominator}
\label{fig:iterate-single-entry}
\end{figure}

We compose a larger \SESE{} by merging two smaller \SCoP{s}. In order to merge two
\SESE{s}, as described in Figure-\ref{fig:merge-sese}, to form a new \SESE{,} we
search the nearest common dominator $dom$ and the nearest common post-dominator
$pdom$ to form a new region.  After that we find the nearest dominator of $dom$
with single entry because we want to build an \SESE{.}  We iterate on the dominator
tree until we find such basic block, as described in
Figure-\ref{fig:iterate-single-entry}.  If any of the dominators has two
predecessors but one of them is a back edge, then that basic block also
qualifies as a dominator with single entry.  Similarly, we find nearest
post-dominator of $pdom$ with single exit.  For this, we iterate on the
post-dominator tree until we find such basic block.  If any of the
post-dominators has two successors but one of them is a back edge, then that
basic block also qualifies as a post-dominator with a single exit.

After such entry and exit edges have been found, we check whether the entry
basic block of the region, which is the destination of the entry edge
$entry.dest$ dominates the exit basic block of the region, which is the source
basic block of the exit edge $exit.src$.  Similarly, $exit.src$ should
post-dominate the $entry.dest$.  Also, the bounding basic blocks -- source
basic block of the entry edge and the destination basic block of exit edge --
should belong to the same loop depth.  It is possible to continue extending the
region by finding edges satisfying both these conditions, although for now the
algorithm chooses to bail out.  We would like to extend this functionality in the
future.

If all the previous constraints are satisfied, the algorithm returns a larger
\SESE{} which subsumes both the \SCoP{s}, otherwise it returns an invalid \SESE{} to
inform that the merge was unsuccessful.

\subsection{Analysis of the \SCoP{} detection algorithms}
The new algorithm is linear in the number of loops as it iterates on the
natural loops tree.  The number of calls to the dominators and post-dominators
is also linear in the number of loops.

The previous implementation of \SCoP{} detection in Graphite is linear in the
number of \CFG{} edges as it discovers regions by walking on the \CFG{.}  In the
implementation of Polly, the use of iterated dominance frontiers to build the
\SESE{} regions tree may lead to quadratic behavior in some cases \cite{ramalingam}.
This can be expensive when the function body is large, specially with aggressive
inlining.

The complexity of the validity function is still a point that could be improved
in all the \SCoP{} detection algorithms: every statement of the \SESE{} has to be
validated.  When a \SCoP{} is extended upwards, either including in the larger
region sequential loops, or going from an inner loop to an outer loop, the \scev{}
instantiation point changes, and thus the \scev{s} of inner or lower regions become
invalid under the new instantiation point, and have to be analyzed again.

The new \SCoP{} detection algorithm helps analyze fewer statements in case of an
invalid \SCoP{} because it focuses on the structure of the natural loops first
rather than the validity of each statement.

\section{Experimental Results}

\begin {figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{polybench/speedup}}
  \end{center}
  \caption{Speedup of improved \SCoP{} detection on Polybench.}
  \label{fig:polybench-speedup}
\end {figure}

\begin {figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{gcc/speedup}}
  \end{center}
  \caption{Overhead of the new and old \SCoP{} detection on the overall compilation
    time when compiling the code of \GCC{} 6.0.}
  \label{fig:gcc-speedup}
\end {figure}

To compare against the existing \SCoP{} detection algorithms, we set up two
experiments: first we look at a static metric consisting in counting the number
of \SCoP{s} and the number of loops in those \SCoP{s} discovered on a set of
benchmarks known to contain loops that benefit from polyhedral compilation.
Then we evaluate how much time the compiler spends on trying to find \SCoP{s}
containing meaningful loop nests to be optimized in the polyhedral compilation
on a large code-base.

\subsection{\SCoP{} metrics on Polybench}
To validate the impact of our changes to the Graphite framework by replacing the
old \SCoP{} detection algorithm with the new one, we evaluate the number of \SCoP{s}
and number of loops per \SCoP{} discovered on the Polybench \cite{polybench} in
Table-\ref{tab:scop-metrics}.  We also provide the same metrics for Polly, and
we intentionally do not provide conclusions based on these metrics because a
fair comparison on these metrics is difficult: the pass ordering and level in
the pass pipeline at which Graphite and Polly apply are very different, and so
the shapes of the \CFG{} and natural loops tree on which the \SCoP{} detection applies
are radically different.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Metric                   	& New  & Old  & Polly  \\
      \hline
      \SCoP{s}          		& 34   & 189  & 30     \\
      Max loops/\SCoP{}           	& 17   & 8    & 11     \\
      Min loops/\SCoP{}           	& 2    & 1    & 2      \\
      \SCoP{s} with min loops/\SCoP{} & 7    & 109  & 3      \\
      Loops in \SCoP{s} 		& 207  & 316  & 155    \\
      Loops/\SCoP{}               	& 6.09 & 2.59 & 5.17   \\
      \hline
    \end{tabular}
  \end{center}
  \caption{\SCoP{} metrics on Polybench.}
  \label{tab:scop-metrics}
\end{table}

There were no regressions in Graphite while moving from the old to the new \SCoP{}
detection because the difference: $316 - 207 = 109$, corresponds
to \SCoP{s} with only one loop.  The new \SCoP{}
detection discards all \SCoP{s} with less than $2$ loops.

We also see that the new algorithm discovers larger \SCoP{s} on an average,
i.e., from $2.59$ to $6.09$ loops per \SCoP{.} This is because, the new
algorithm allows \SESE{} without a surrounding loop to be a \SCoP{} so that two
adjacent outermost loops can be a \SCoP{,} which was not possible with the old algorithm
(Section-\ref{subsec:graphite-SCoP}). Now the largest \SCoP{} discovered has $17$
loops whereas, with the old \SCoP{} detection the largest \SCoP{} only contained $8$ loops.

\subsection{Evaluation of compilation time overhead}
The current infrastructure to measure the compilation time in \GCC{} isn't precise
enough to benchmark portions of passes which are not dominating the overall
compilation time.  On a recent x86\_64 machine, the overall compilation time
spent in Graphite was a fraction of a second for each benchmark
in Polybench \cite{polybench}.

We used Valgrind to get more precise data on the number of instructions executed
by the \SCoP{} detection algorithms.  The command used is:
\begin{verbatim}
$ CFLAGS="-Ofast -fgraphite-identity"
$ valgrind --dsymutil=yes --tool=callgrind \
    --dump-instr=yes cc1plus $CFLAGS $file
$ callgrind_annotate --threshold=100 \
    --inclusive=yes callgrind.out
\end{verbatim}

The output of callgrind\_annotate lists all the functions in the execution of
the program together with a count of the number of instructions executed by the
CPU.  The option ``--inclusive=yes'' allows us to gather the total number of
instructions executed in a function and all the functions called from it.  We
thus report the number of instructions of the top-level functions of the old and
new \SCoP{} detectors, respectively build\_scops and build\_scop\_depth.

Table-\ref{tab:overall-insns} presents the overall number of instructions
executed in the new and old \SCoP{} detection on several benchmarks: Polybench
\cite{polybench}, a large C++ application Tramp3d-v4 \cite{richi}, and all the
source files of \GCC{} 6.0.  Columns New and Old report the number of instructions
executed by the old and new \SCoP{} detectors of \GCC{;} Speedup reports the speedup
between Old and New; Main reports the cumulative number of instructions executed
by the main function of the \GCC{} cc1plus compiler; Old \% and New \% report the
overall compilation time overhead of the old and new \SCoP{} detection.  On large
applications, the speedup over the old algorithm for \SCoP{} detection is very
important: on Tramp3d-v4, the old \SCoP{} detection accounted for $7.0\%$, and
$0.3\%$ in the new \SCoP{} detection.  On Polybench there is some slowdown
corresponding to the detection of larger \SCoP{s}: every step in enlarging the \SCoP{}
may trigger extra computations for \scev{s}.  The slowdowns only correspond to
benchmarks with loops that can be optimized by a polyhedral compilation.

Figure-\ref{fig:polybench-speedup} reports the speedup in \SCoP{} detection when
compiling the $32$ files of Polybench, and Figure-\ref{fig:gcc-speedup} has the
speedup in \SCoP{} detection for the $578$ files of the source code of \GCC{} 6.0.

\begin{table}[h!]
  \begin{center}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      Benchmark  & Old      & New        & Speedup & Main       & Old \% & New \%  \\
      \hline
      Polybench  & $3.3e^8$ & $4.8e^8$   & $0.7$   & $2.5e^{10}$ & $1.4$  & $1.9$   \\
      Tramp3d-v4 & $1.8e^9$ & $6.2e^8$   & $2.8$   & $1.9e^{11}$ & $7.0$  & $0.3$   \\
      \GCC{} 6.0    & $1.5e^{10}$ & $6.7e^8$ & $22.6$  & $6.1e^{12}$ & $0.24$ & $0.01$  \\
% Polybench: old=333775317, new=475127633, main=
% Tramp3d-v4: old=1755147630, new=624729992, main=
% \GCC{}: old=15072748886, new=667787862, main=6101635653835
      \hline
    \end{tabular}}
  \end{center}
  \caption{Overall number of instructions spent in \SCoP{} detection.}
  \label{tab:overall-insns}
\end{table}

\section{Conclusion and Future Work}
We have shown that our new algorithm of \SCoP{} detection is faster in terms of
compilation time and that it detects larger \SCoP{s} than the previous
implementation in Graphite.  This stems from the fact that operating on a
higher-level program representation allows the algorithm to converge faster on
regions that cannot be represented in the polyhedral model.

The improvements in compilation time assure that enabling the polyhedral
optimizations by default at common optimization levels will not slow down the
compilation time for the majority of programs compiled by an industrial
compiler: i.e., programs with no loops, or with sparse loops that should not be
translated into the polyhedral model.

While the new algorithm focuses only on detecting \SCoP{s} with relevant loops,
it still tries to maximize the size of the \SCoP{.}  This may lead to increased
polyhedral compilation time.  We think that the \SCoP{} detection should take a
more active role in driving the polyhedral optimizations by analyzing the shape
of the \SCoP{s} and selecting appropriate polyhedral optimizations.

Another way to improve the compilation time of an industrial polyhedral
compiler, that is not discussed in this paper, is by using the profile
information collected during an earlier execution of the program and made
available to the compiler during subsequent compilation: one could use the
profile information to tune the \SCoP{} detection algorithm to focus only on the
hot paths of the program.  We are also investigating techniques to tune the
amount of compilation time allocated to the polyhedral optimizer (isl
\cite{verdoolaege2010isl} in case of gcc).

\section{Acknowledgements}
We would like to thank Samsung Austin R\&D Center for supporting our work on
improving Graphite and Polly.  We would also like to thank Tobias Grosser,
Johannes Doerfert, and Michael Kruse for discussions on improving the existing
\SCoP{} detection in Polly.

\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
