\title{Enabling Polyhedral optimization for wide use}

\abstract{
  For the past few years graphite framework, which brought polyhedral optimizations
  to gcc, was largely unmaintained. We made importatnt designed changes and fixed bugs accumulated over
  the past. The transition from cloog to isl left dead code in multiple places, and made some optimizations
  redundant. We cleaned up that code, so graphite codebase became smaller after recent changes.
  We enabled graphite to detect wider range of loops which it can optimize and discard
  difficult-to-optimize loops in a much faster way. We have also brought demand driven optimization
  into the graphite framework, since we want it to spend extra time in loops which matters,
  in the presence of profile information.
  Demand driven optimization is new approach to optimization: Moving from problem to solution.
}

\section{Goals}
Enabling widespread usage of graphite by having one flag to the end user.
Fixing accumulated bugs.
Substantial improvements on several benchmarks with no significant regression.
Propose enabling of graphite by default under one optimization flag.
Limiting the number of computations to reduce compile time.

\section{faster scop detection for selecting relevant loop nests}
The scope of the polyhedral program analysis and manipulation is a sequence of loop
nests with constant strides and affine bounds. It includes non-perfectly nested loops
and conditionals with boolean expressions of affine inequalities.
The maximal Single-Entry Single-Exit (SESE) region of the Control Flow Graph
(CFG) that satisfies those constraints is called a Static Control Part (SCoP). \ref{Girbal, Bondhugula, Trifunovic}

Current algorithm for SCoP detection in graphite was based on dominator tree where a tree (CFG) traversal is required
for analyzing an SESE. The tree traversal is linear in the number of basic blocks and SCoP detection is linear in
number of instructions. This is reasonably fast but it utilizes a generic infrastructure of SESE. With regards to
polyhedral optimization we are only interested in subtrees with loops. So it makes more sense to utilize higher level
semantics of CFG e.g., loop tree. Since higher level abstractions contain more statments, discarding them early
makes algorithm converge faster.

The new algorithm is geared towards tree traversal on loop structure. The algorithm is linear in number of loops
which is much faster than the previous algorithm. The algorithm is based on a very simple structural property of loop
tree.

LoopTree = LoopNest
LoopNest = LoopTree; nested loop

Briefly, we start the traversal at a loop-nest and analyze it recursively for validity. Once a valid loop is
found we find a valid adjacent loop. If an adjacent loop is found then we merge both loop nests
other wise we form a SCoP and resume the algorithm from the adjacent loop nest. The data structure to represent an SESE
is an ordered pair of edges (entry, exit). Choosing a simple data structure allows us to extend a SCoP in both the
directions. With this approach, the number of instructions to be analyzed for validity reduces to a minimal set.
We start by analyzing those statements which are inside a loop, because validity of those statements is
necessary for the validity of loop. The statements outside the loop nest can be just excluded from the
SESE if they are not valid.

In the graphite framework we do not include statements outside before the first and after the last loop in an SESE.
So in that sense, SCoP detected by this function may not be maximal.

GIMPLE statements belonging to the SCoP should not contain calls to functions with
side effects (pure and const function calls are allowed) and the only memory references
that are allowed are accesses through arrays with affine subscript functions.

To make the scop detection simpler we canonicalize the loops into a closed SSA form.
The previous graphite framework detected scop by analyzing an SESE. That approach resulted in a highly recursive structure
with redundancies at several places and hence, increase in compile time. In the new framework, we try to discard irrelevant
regions as fast as we can. Essentially,
\begin{enumerate}
\item Discard functions with less than two loops: Since we are mostly intersted in optimizing loop nests
or loop which has at least one sibling.
\item Start the scop detection from a CFG node which begins from a loop header.
\item Break the SCoP at a point where we find a statement which cannot be represented by graphite.
\item 
\end{enumerate}

\section{Removing limit_scops}
The functionality limit_scop was added
as an intermediate step to discard the loops which graphite could not
handle. Removing limit_scop required handling of different cases of loops and
surrounding code.  The scop is now larger so most test cases required 'number of
scops detected' to be fixed. By increasing the size of scop we can now optimize
loops which are 'siblings' of each other. This could enable loop fusion on a
number of loops.



\section{Internal parameters}
A parameter in SCoP is defined as any operand which is defined outside. These are first order parameters.
However, there are statements which only use parameters to define another operand. We call these as internal or
derived parameters.
IP = F (scop-parameters)
Since IPs can be derived from SESE parameters, they also do not require renaming when SESE is copied for polyhedral
transformations. While copying SESE we copy internal parameters at the beginning of the SCoP.
We maintain relative ordering of the internal parameters only as their ordering w.r.t. other
statements (which do not define internal parameters) does not affect the semantics of the program.
We could chose to copy IPs outside of the SCoP as well but we don't so as to preserve the original structure of the
program in case no polyhedral transformations were applied. This also preserves the live-range
of the original program and hence the register pressure.


\section{Fixing accumulated bugs}
For the past few years graphite was not actively maintained and accumulated bugs. We also believe that
moving from cloog to isl might have exposed a few corner cases of the initial implementation of graphite.
Even the graphite-identity was not working. graphite-identity is not intended to change the functionality
of the program. So the first step was to get graphite-identity to work. Second was to remove optimizations which
were either redundant because of transition to isl, or were generating incorrect code. Also, the feedback that we
got from the community was that graphite needs to operate under one flag otherwise users might not use it effectively.
The onus of generating optimal code should be on the framework and it should decide the best code to generate
for optimal data locality. So we deprecated all the flags except -fgraphite-identity and -floop-nest-optimize.
We kept -fgraphite-identity for debugging purposes only.

\section{Limiting the number of computations done in ISL}
Newer releases of ISL have a way to bail out when total number of computations reach a limit.
This feature is very useful because this way we can limit the overhead in compile time. This also
allows greater control over optimization when loops are not too important. We provided a flag so
that users can tune it as required.


\section{Translating gimple IR to polyhedral representation without changing the original IR}
The gimple IR needs to be translated to polyhedral representation to be fed to isl.

This is done by first creating a copy of original SCoP and conditionally enabling one region.
Since the IR is in SSA form copying it requires a lot of book-keeping for scalars and PHIs.
Existing algorithm in graphite got around this problem by translating scalar to a one element array.
Since array is a data-reference, it does not require SSA renaming and book-keeping. This choice,
however, causes data references created in the original code. So even when there was no optimization done
IR was modified. Current algorithm removes this limitation by representing scalars as one element array only in the
polyhedral representation (virtually). This algorithm is more involved but avoids changes in the original code.

\subsection{Interesting scalars}
\begin{enumerate}
        \item Cross basic block scalar dependencies.
        \item Reads from outside of SCoPs
        \item Writes to outside of SCoP.
\end{enumerate}

\subsection{Build scattering polyhedrons from the program schedule}
Static and dynamic schedule are mapped in odd/even dimensions respectively.
The schedule can be improved by minimizing the number of dimensions as done in polly-llvm.

\section{Translating ISL polyhedral representation to gimple IR in SSA}
There are situations where it is not possible to map the resulting code back to the original.
\subsection {Translating PHIs}
\begin{enumerate}
  \item Cond PHI nodes. Finding the right direction from where SSA names/constants are coming. Issues with dominating
    preds.
  \item Loop PHI nodes. Finding the right basic block to insert loop PHIs.
  \item Loop closed-PHI nodes. Inserting closed-PHIs at all the merge points.
\end{enumerate}
Disambiguating SSA names while mapping back.



\section{Measuring effectiveness of optimization}
Some way/heuristic of measuring 'amount' of optimization is reqruired because it is a costly optimization.
By having a metric would allow us to bail out early.
\begin{enumerate}
  \item Proximity
  \item Complexity in terms of code-size
\end{enumerate}

Bail out early based on following parameters.
\begin{enumerate}
  \item Optimization wasn't effective.
  \item Computation taking too long.
  \item Hotness of code.
  \item Code size.
  \item Number of parameters
\end{enumerate}

Essentially,

Number of ISL computations N = F (hotness, codesize, number of params, threshold of opt)
N increases with hotness, threshold of opt.
N decreases with codesize, number of params.

\section{Removing redundant dimensions from polyhedral represenation for faster compilation}

\section{Representing static schedule in ISL schedule node}

\section{Loop canonicalization}
Currently, we insert an empty basic block after each loop in a function. This is done so that
we can add PHIs to converge definitions from SCoPs from gimple and poly side. This changes
the IR but those changes are easily removed by subsequent passes. So it does not cause any
performance/code-size change in the generated code.


\section{Implementing loop fusion}

\section{Enabling graphite with profile guided optimization}
Although we have tried to redesign graphite for faster compile time, it still has some overhead.
Experimental results show an average of {} overhead in compile time.
Because of that, we would like to enable this for cases when compile time is not of much concern,
e.g. benchmarking, iterative compilation with feedback.

We have also enabled graphite to spend extra time on important loops. This can be done by tuning
isl-timeout programmatically based on the hotness of the loop. We believe expensive optimizations
can have the option to be demand driven so that compiler spends time on program portions which matters
most from the performance perspective. \ref{Duesterwald}


\section{Experimental Results}

\section{Conclusion and Future Work}
Iterative compilation with varying threshold of computations, and with profile info.


\references{}

S. Girbal, N. Vasilache, C. Bastoul, A. Cohen, D. Parello, M. Sigler, and O. Temam.
Semi-automatic composition of loop transformations for deep parallelism and memory
hierarchies. Intl. J. of Parallel Programming, 34(3):261–317, June 2006. Special issue on
Microgrids


U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A practical automatic
polyhedral parallelization and locality optimization system. In ACM SIGPLAN Conf. on
Programming Languages Design and Implementation (PLDI’08), Tucson, AZ, USA, June
2008.

Trifunovic, Konrad, et al. "Graphite two years after: First lessons learned
from real-world polyhedral compilation."
GCC Research Opportunities Workshop (GROW'10). 2010.


Duesterwald, Evelyn, Rajiv Gupta, and Mary Lou Soffa. "Demand-driven computation of interprocedural data flow." Proceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages. ACM, 1995.
http://www.cs.ucr.edu/~gupta/research/Publications/Comp/popl95.pdf

