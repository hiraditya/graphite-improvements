\documentclass{sigplanconf}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{Impact'16}{January 18--20, 2016, Prague, Cz}
\copyrightyear{2016}
\doi{nnnnnnn.nnnnnnn}

\title{SCoP Detection: A Fast Algorithm for Industrial Compilers}
\authorinfo{Aditya Kumar, Sebastian Pop}
\authorinfo{Samsung Austin R\&D Center}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Loop optimizations are usually described on languages like Fortran where loops
and arrays are well behaved syntactic constructs.  In contrast with Fortran, low
level languages like C or C++ do not offer the same straight forward ease in
analysis of loops, induction variables, memory references, and data dependences,
essential to all high level loop transforms.  Compilers for low level languages
like GCC and LLVM lower the statements and expressions into low level constructs
common to all imperative programming languages: loops are represented by their
control flow, i.e., jumps between basic blocks, memory references are lowered
into pointer accesses, and expressions are lowered into three address code.
Thus, high level C++ expressions containing exceptions are represented on a low
level representation by an explicit complex control flow, that was implicit in
the high level language.

From a practical point of view, optimization passes in compilers like GCC and
LLVM are implemented on this low level representation, in order to avoid
duplicating analyses and optimizations for each supported language, and in order
to abstract away from the specifics of each language: for instance, consider the
semantics variability of loop statements in Fortran, C, C++, Java, and Ada
languages, all currently compiled and optimized by the middle-end of GCC.  The
price paid for the generality of the approach is extra compilation time spent in
recognizing all the high level loop constructs from the low level
representation.  This paper describes a fast algorithm to discover, from a low
level representation, loops that can be handled and optimized by a polyhedral
compiler.

\subsection{What are the boundaries of a SCoP?}

Regions of code that can be handled in the polyhedral model are usually called
Static Control Parts \cite{Bondhugula}, abbreviated SCoPs.
Usually, SCoPs may only contain regular
control flow void of exceptions or other constructs that may provoke changes in
control flow such as condition expressions dependent on data read from memory or
side effects of function calls.  As the compiler cannot easily handle such
constructs in the polyhedral model, these statements will not be integrated in a
SCoP, causing a split of the region containing such difficult statements into
two SCoPs.

To extend the applicability of polyhedral compilation, \cite{scopExtend}
presents techniques to represent general conditions, enlarging the limits of
SCoPs to full function body.  We will not consider these SCoP extension
techniques in the current paper, as we want a fast SCoP detection suitable to be
turned on by default in usual optimization levels, like ``-O2'', ``-O3'', or
``-Ofast''.  For that, we need an algorithm that is able to quickly converge on
the largest SCoPs that can profitably be transformed by a polyhedral compiler
like ISL \cite{verdoolaege2010isl} within a reasonable amount of compilation time.
Practically, on a large number of compiled programs, we want to use close to
zero overhead for functions without loops, or non interesting loops for the
polyhedral compilation, and less than ten percent of the overall compilation
time when optimizing loops for hot paths as shown in the profile of the compiled
program.

\subsection{High level overview of SCoP detection algorithms}

Existing implementations of SCoP detection based on low level representations
are based on some form of representation of the control flow graph: the first
implementation we did for Graphite \cite{graphite} was based on the control
flow graph itself and the basic block dominators tree.  Then Polly improved on
this SCoP detection \cite{polly} by working on an abstraction of the control flow graph, the
single entry single exit (SESE) region tree, a representation that integrates
the dominators tree on the control flow graph \cite{sese}.

In order to avoid constructing the region tree, and to help the SCoP detection
algorithm to faster converge on the parts of code that matter to polyhedral
compilation, we used an abstraction of the control flow graph, the natural loops
tree \cite{dragonbook}, together with dominance information.  In terms of
compilation time, both representations come at zero cost, as they are maintained
intact and correct between GCC's middle-end optimization passes, lowering the
compilation cost for the most numerous functions compiled by GCC: functions with
no loops, or with non contiguous isolated loops that cannot be profitably
transformed in a polyhedral compilation.

\subsection{Contributions of this paper}
\begin{enumerate}
  \item We present a new algorithm for SCoP detection, that improves over the
    existing techniques of extracting SCoPs from a low level intermediate
    representation.
  \item We provide a comparative analysis of the earlier algorithm implemented
    in Graphite, the SCoP detection as implemented in LLVM-Polly, and the new
    SCoP detection that we implemented and integrated in GCC 6.0.
  \item We provide an analysis of the shape of SCoPs detected by these three
    algorithms in terms of number of SCoPs detected in benchmarks, and number of
    loops per SCoP.
  \item Finally, we present experimental results on compile time improvements.
\end{enumerate}

\newpage
\section{Comparative analysis of SCoP detection algorithms}

We first describe the SCoP detection algorithms implemented originally in
Graphite and then in Polly, and then we show how the new algorithm improves over
these earlier implementations.  The common point of all these algorithms is that
they work on a low level representation of the program.  Analysis passes are
building more synthetic representations as described in the next subsection.

\subsection{Code analysis: from low level to higher level representations}

Starting from a low level representation of the program, a compiler extracts
information about specific aspects of the program through program analysis:

\begin{itemize}
\item The control flow graph (CFG) \cite{dragonbook} is built on top of a goto
  based intermediate representation: the nodes of the CFG represent the largest
  number of statements to be executed sequentially without branches, and the
  edges of the CFG correspond to the jumps between basic blocks.

\item The basic block dominator (DOM) and post-dominator (Post-DOM) trees
  \cite{dragonbook, ramalingam} represent the relation between basic blocks with respect to
  the control flow properties of the program: a basic block A is said to
  dominate another basic block B when all execution paths from the beginning of
  the function have to pass through A before reaching B.  Similarly, the
  post-dominator information is obtained by inverting the direction of all edges
  in the CFG and then asking the same question with respect to the block ending
  the function: A is said to post-dominate B when all paths from the end of the
  function have to pass through A to reach B in the edge-reversed CFG.

\item The single entry single exit (SESE) regions \cite{sese} are
  obtained from the CFG and the DOM and Post-DOM trees by identifying contiguous
  sequences of basic blocks dominated and post-dominated by unique basic blocks.
  The SESE regions may contain sub regions that have the SESE property: this
  inclusion relation is represented as a tree.

\label{subsec:loop-tree}
\item Natural loops \cite{dragonbook, ramalingam} are detected as strongly connected
  components (SCC) \cite{tarjan} on the CFG, loop nesting and sequence are
  represented under the form of a tree: loop nodes are linked with $inner$ loop,
  and $next$ loop relations.  The function body is represented as a loop at the
  root of the loop tree, and its depth is zero.  The depth of inner loops is one
  more than their parent, and sibling loops linked through $next$ are at the
  same depth.  When the CFG contains an SCC that is not reducible to a natural
  loop, for example two back-edges pointing back to a same basic block, all the
  edges and nodes of the CFG involved in that SCC are marked with a flag
  IRREDUCIBLE\_LOOP.

\item Static Single Assignment (SSA) form \cite{cytron}, inserts extra scalar
  variables such that each definition is unique.  The assignments to scalar
  variables are either the result of expressions, or the result of phi nodes
  placed at control flow junctions in basic blocks that are target of two or
  more control flow edges.  Assignments from phi nodes represent all the
  possible assignments considering the control flow graph.

\item An abstraction of the SSA is that of a declarative language
  \cite{spop2007} in which there are no assignments or imperative language
  constructs.  The abstraction only represents the computation of scalar
  variables that are either the result of arithmetic expressions, or the result
  of two kinds of phi nodes: loop phi nodes have two arguments, one is defined
  outside the loop containing the phi node, and the other is defined inside the
  loop.  Loop close phi nodes have only one argument that is defined in an inner
  loop.

\item The analysis of scalar evolutions (scev) \cite{scev} starts from the
  abstraction of the SSA representation described above by recognizing the
  evolution function of scalar variables for loop phi nodes, loop close phi
  nodes, and derived scalar declarations.  Loop phi nodes are declared by an
  initial value and an expression containing a self reference, when the self
  reference appears in an addition expression together with a scalar value or an
  invariant expression in the current loop, the scev represents a recursive
  function with linear or affine evolution.  Loop close phi nodes are declared
  as the last value computed by an expression that may variate in a loop, the
  scev then represents a partial recursive function.  All other scalar
  declarations can be expressed as scevs derived from declarations of other
  recursive and partial recursive functions.

\item The analysis of the number of iterations \cite{scev} provides a scev that
  represents the number of times a loop is executed.  The number of iterations
  is computed as the scev of a close phi node, or last value, of a scev starting
  at zero and incremented by one at each iteration of the loop.  The number of
  iterations can also be computed by ISL, as we provide the scevs of all the
  variables involved in each condition that we translate.

\item Pointer analysis detects base and access functions for all memory
  locations that are accessed in the program.

\item Alias analysis disambiguates the base pointers and identifies which
  pointers may or do not access the same memory locations.

\item Data reference analysis uses the results of the scev analysis to determine
  the memory access patterns of pointers in loops.

\item The delinearization analysis \cite{delinearization1, delinearization2}
  reconstructs a high level representation of arrays under the form of Fortran
  subscripts.  The delinearization uses all the access functions of all the data
  references in a loop, a region, or a function, in order to make sure all
  memory accesses follow the exact same pattern, i.e., the same subscript
  dimensions are valid in all accesses.
\end{itemize}

As we have seen, gradually, these analyzes extract from low level constructs
higher level representations: later analyzes are based on earlier lower level
results, building up a castle out of basic bricks.  All this information
synthesized by the compiler allows the representation of diverse imperative
programming languages into a polyhedral form \cite{Girbal} containing a very
high level information of loop iteration domains, memory accesses, and static
and dynamic schedules.

To reduce compilation time, the SCoP detection algorithms start from the lowest
level analysis results that are commonly available and try to quickly discard
parts of code that either cannot be translated in the polyhedral model or that
cannot be profitably transformed.  The compiler has to quickly evaluate whether
a part of the code is amenable to translation in the polyhedral model before
spending time in computing higher level costly information.  For that reason,
information about the CFG structure, and the number of loops per function are
the first checks in the SCoP detection algorithms, followed by a linear walk
over all the statements of the region of code to ensure the absence of side
effects in statements, and to gather more costly information about the number of
iterations, the linearity of memory access functions and condition expressions.

\subsection{Former algorithm for SCoP detection in Graphite}
\label{subsec:graphite-SCoP}
The first Graphite SCoP detection algorithm was implemented on a very low level
representation of CFG and DOM \cite{trifunovic}.  These representations were too
restrictive for the scev analysis to be able to determine the loops to be
considered as variant and the loops that have to be considered as invariant, in
which the scev analysis should not analyze and instantiate further scalar
variables in order to consider them as parameters.  This resulted in a very
restrictive limitation of SCoPs that had to have one full loop fully contained
in the SCoP: for instance a sequence of two loops with no surrounding loop would
not be represented as a SCoP and the SCoP detection would split these two loops
into two distinct SCoPs.  This limitation has been removed in GCC 6.0 by the
improvements to the SCoP detection described later in this paper.

\subsection{Polly SCoP detection: based on SESE region tree}
\label{subsec:polly-SCoP}
Polly's SCoP detection is based on an analysis of SESE regions.  The discovery
of all the regions in a function may be expensive, specially when the number of
basic blocks in its CFG is very large.  In the current LLVM implementation of
SESE region discovery, the use of dominance frontiers may have quadratic
behavior in some cases.  The algorithm described in this paper may help in
reducing the cost of the SESE region analysis by replacing the use of dominance
frontiers with a simpler check based on the natural loops properties of depth
levels and regions of the CFG corresponding to irreducible loops, see
Figure-\ref{fig:merge-sese} in Section-\ref{sec:new-SCoP-detection}.

\subsection{Maximal SCoPs and why they might not be very useful}
\label{subsec:maximality}
A maximal SCoP is the largest region satisfying all the properties of a SCoP and
which cannot be further extended.  This concept is currently used in Polly
\cite{polly} and was used in the previous SCoP detection of graphite
\cite{graphite}.

Since the main focus in the polyhedral model is the transformation of loops, all
code translated in the polyhedral model that is not part of a loop, or part of a
sequence of loops, only constitues an overhead in compilation time.  Finding and
representing a maximal SCoP is not necessary as it is possible to analyze all
the surrounding conditions and extract constraints on the parameters.

To illustrate our point, consider the maximal SCoP in
Figure-\ref{fig:maximality}: suppose that the region from IfCondition to EndIf
satisfies the properties of a SCoP, and contains a single loop composed of a
LoopHeader and a LoopLatch.  All the other blocks of a maximal SCoP
IfCondition, TrueRegion, and EndIf add extra overhead to the polyhedral
compilation without real benefit in terms of adding opportunities for loop
optimizations.  Having a SCoP only containing the loop would prove more
beneficial in terms of optimizations and overall compilation time.

\begin{figure}
\centering
%rankdir=LR;
\digraph[scale=0.5]{abc}{
BeforeSESE -> IfCondition
IfCondition -> TrueRegion
IfCondition -> LoopHeader
LoopHeader -> LoopLatch
LoopLatch -> LoopHeader
LoopHeader -> EndIf
TrueRegion -> EndIf
EndIf -> AfterSESE
}
\caption{Maximal SCoP bounded by IfCondition and EndIf}
\label{fig:maximality}
\end{figure}

\section{A new faster SCoP detection to select relevant loop nests}
\label{sec:new-SCoP-detection}
The new algorithm for SCoP detection works by induction on the structure of the
natural loops tree as described in Section-\ref{subsec:induction} and listed in
Figure-\ref{fig:induction}.  The traversal of the loop tree tries to enlarge a
region by attaching adjacent or outer valid regions as described in
Section-\ref{subsec:merge-sese} and listed in Figure-\ref{fig:merge-sese}.
Section-\ref{subsec:loop-tree} has a brief introduction to the notion and
properties of natural loops, dominators, and post-dominators properties used in
this section; we refer our readers to \cite{ramalingam} for an in-depth
discussion.

The algorithm assumes following preconditions during the SCoP detection:
\begin{itemize}
\item The natural loops tree represents a reducible CFG: all edges and basic
  blocks part of an irreducible SCC are tagged with an IRREDUCIBLE\_LOOP flag.
% I am not sure we need these properties for the scop detection: I think we need
% this when we start translating to the polyhedral representation.

%\item All the loops are in closed SSA form. Since GCC does not guarantee
%loops to be in closed SSA form, we canonicalize each loop in closed SSA form
%before starting the SCoP detection process.
%\item Alias information is available.
\item Dominance information is available.
\item Mechanism to compute the evolution of scalars in a region.
\end{itemize}

The alias analysis, dominance, and scalar evolution are already available in
GCC.  We recalculate the DOM because we canonicalize each loop before detecting
the SCoPs.  Since SCoP detection is like an analysis pass, it preserves the DOM,
alias, and the scalar evolution of data references.

\subsection{Induction on the structure of natural loops}
\label{subsec:induction}

\begin{figure}
\begin{verbatim}
build_scop_depth (s1, loop):
  s1 = build_scop_depth (s1, loop->inner)
  s2 = merge_sese (s1, get_sese (loop))
  if (s2 is an invalid scop)
    {
      // s1 might be a valid scop, so return it
      // and start analyzing from the adjacent loop.
      build_scop_depth (invalid_sese, loop->next)
      return s1
    }
  if (loop is an invalid scop in s2)
    return build_scop_depth (invalid_sese, loop->next)
  return build_scop_breadth (s2, loop)

build_scop_breadth (s1, loop):
  sese_l s2 = build_scop_depth (invalid_sese, loop->next)
  if (s2 is an invalid scop)
    {
      if (s1 is a valid scop)
        add_scop (s1)
      return s1
    }
  combined = merge_sese (s1, s2)
  if (combined is a valid scop)
    s1 = combined
  else
    add_scop (s2)
  if (s1 is a valid scop)
    add_scop (s1)
  return s1
\end{verbatim}
\caption{Induction on the structure of natural loops}
\label{fig:induction}
\end{figure}

The traversal of the natural loops tree starts at a loop-nest at depth one.  The
code inside the loop and all its nested loops are analyzed recursively for
validity, as described in Section-\ref{subsec:validity} and
Figure-\ref{fig:induction}.  Once all the validity constraints are satisfied,
the loop-nest becomes a valid SCoP and it is saved in a set of
already found SCoPs. While adding a new SCoP to the set of detected SCoPs, we
first remove any SCoP which are either a sub-SCoP (completely surrounded) or
intersects (partially overlaps), with the new one. The way algorithm runs, from
bottom up for each loop nest, any new SCoP to be added cannot be subsumed by an
existing SCoP in the set. This way, the set of SCoPs maintained in the set are
mutually exclusive w.r.t. the regions they span i.e., no SCoP intersect with
another in the set of detected SCoPs.

After a valid loop is found, the algorithm analyzes the next loop, a loop at
same depth and immediate sibling of the loop just analyzed.  If an adjacent loop
is found to be a valid SCoP, we try to merge both loop nests as described in
Section-\ref{subsec:merge-sese} and Figure-\ref{fig:merge-sese}.  If a combined
SESE has been found, which subsumes both the SCoPs, it is analyzed for
validity. Even if we have already analyzed the statements in the combined SESE
in their respective sub-SCoPs, we need to re-analyze them because the scalar
evolution of the data references change with the region of the program under
analysis. If the combined SESE represents a valid SCoP, then it is saved, after
removing any intersecting or sub-SCoPs, and further analysis is continued to
extend the SCoP again.  If no such SESE could be found, the algorithm keeps them
as two separate SCoPs continues by trying to extend the second SCoP.

With the new approach it is faster to discard many invalid loop-nests early. The
algorithm analyzes statements which matter most by starting the SCoP detection
from a loop-tree node (CFG node which begins from a loop header). This allows
discarding unrepresentable loops early in the SCoP-detection process, thereby
discarding SESE region surrounding them.  This way, the number of instructions
to be analyzed for validity reduces to a minimal set.  We start by analyzing
those statements which are inside a loop, because validity of those statements
is necessary for the validity of loop.  The statements outside the loop nest can
be excluded from the SESE if they are not valid.  Since this algorithm starts
from the loop header, it could exclude statements before the first, and after
the last loop in an SESE.  Also, regions without loops may be excluded if they
are not surrounded by loops.  SCoPs thus detected are not maximal, in contrast
with the example and discussion in Section-\ref{subsec:maximality}.

% Let's not mention this restriction in the paper: we may even revisit this for
% autopar single loops.

%The optimizations enabled in polyhedral compiler ISL mostly work on loop
%nests (e.g., blocking, interchange, tiling etc.), or on sibling loops (e.g.,
%fusion), so we discard functions with less than two loops.

\subsection{Validity checks for a SCoP}
\label{subsec:validity}
An SESE region is regarded as a valid SCoP when it satisfies the following
conditions:
\begin{enumerate}
\item The entry basic block should have only one predecessor and the exit
  basic block should have only one successsor (i.e., an SESE).
\item The entry should dominate the exit.
\item The exit should post-dominate the entry.
\item The scalar evolution of all the operands in the new region should be affine.
\item All the statements inside the region should be reprepresentable in the
  polyhedral model.  Only labels, pure function calls, assignments and
  comparison operations on integers are allowed.
\item It should be possible to represent induction variables (of all the loops)
  as signed integers because ISL might generate negative values in the optimized
  expressions which needs to be code-generated.
\item All the loops in the SESE should have single exits.
\item The number of iterations of the loops in SESE should be deterministic and
  should not overflow.
\item The loop nests in SESE should have at least one data reference.
%\item The SCoP should have at least one data reference
%and no more than
%  `MAX\_DATA\_REFERENCE', a parameter which can be changed by the programmer.
%\item We also limit the number of parameters, references to variables outside
%  the SCoP, to be less than `MAX\_NUM\_PARAMS'. This can also be can be changed
%  by the programmer.
\end{enumerate}

\subsection{Merging SCoPs}
\label{subsec:merge-sese}

\begin{figure}
\begin{verbatim}
merge_sese (first, second):
  ncd: the nearest common dominator
  ncpd: the nearest common post-dominator
  dom = ncd (first.entry, second.entry)
  pdom = ncpd (first.exit, second.exit)
  entry = get_nearest_dom_with_single_entry (dom)
  if (!entry)
    return invalid_sese
  exit = get_nearest_pdom_with_single_exit (pdom)
  if (!exit)
    return invalid_sese;

  // Entry and exit should be in the same loop.
  // Otherwise, the region may contain edges
  // entering or leaving the region, violating
  // the Single Entry Single Exit property.
  if (loop_depth (entry->src->loop_father) !=
      loop_depth (exit->dest->loop_father))
    return invalid_sese;

  // Entry and Exit should not be part of an
  // irreducible loop.
  if (entry.flag == EDGE_IRREDUCIBLE_LOOP
      or exit.flag == EDGE_IRREDUCIBLE_LOOP)
    return invalid_sese

  combined = new_sese (entry, exit)
  if (entry does not dominate exit
      or exit does not post-dominate entry
      or combined is an invalid scop)
    return invalid_sese
  return combined
\end{verbatim}
\caption{Merging two SESE regions}
\label{fig:merge-sese}
\end{figure}

% FIXME: Add checks in GCC for IRREDUCIBLE_LOOP.

\begin{figure}
\begin{verbatim}
get_nearest_dom_with_single_entry (bb):
  if (bb has 1 predecessor e)
    return e
  if (bb has 2 predecessors e1 and e2)
    { // Check for a back-edge
      if (e1.src dominates e2.src)
        return e1;
      if (e2.src dominates e1.src)
        return e2;
    }
  bb = get_immediate_dominator (bb)
  return get_nearest_dom_with_single_entry (bb)

get_nearest_pdom_with_single_exit (bb):
  if (bb has 1 successor e)
    return e
  if (bb has 2 successors e1 and e2)
    { // Check for a back-edge
      if (e1.dest post dominates e2.dest)
        return e1;
      if (e2.dest post dominates e1.dest)
        return e2;
    }
  bb = get_immediate_post_dominator (bb)
  return get_nearest_pdom_with_single_exit (bb)
\end{verbatim}
\caption{Iterative computation of a single entry dominator and a single exit
  post-dominator}
\label{fig:iterate-single-entry}
\end{figure}


% FIXME: Check code in GCC and correct definition of
% get_nearest_pdom_with_single_exit and get_nearest_dom_with_single_entry to
% match the better version here in the paper.

We compose a larger SESE by merging two smaller SCoPs. In order to merge two
SESEs (Figure-\ref{fig:merge-sese} to form a new SESE, we find the nearest
common dominator $dom1$ and the nearest common post-dominator $pdom1$ to form a
new region. After that we find the nearest dominator of $dom1$ with single entry
because we want to build an SESE.  We iterate on the dominator tree until we
find such basic block, as described in Figure-\ref{fig:iterate-single-entry}.
If any of the dominators has two predecessors but one of them is a back edge,
then that basic block also qualifies as a dominator with single entry.
Similarly, we find nearest post-dominator of $pdom1$ with single exit. For this,
we iterate on the post-dominator tree until we find such basic block. If any of
the post-dominators has two successors but one of them is a back edge, then that
basic block also qualifies as post-dominator with single exit.

After such entry and exit edges have been found, we check if the entry basic
block of the region, which is the destination of the entry edge $entry.dest$
dominates the exit basic block of the region, which is the source basic block of
the exit edge $exit.src$. Similarly, $exit.src$ should post-dominate the
$entry.dest$. Also, the bounding basic blocks viz., source basic block of the
entry edge and the destination basic block of exit edge, should belong to the
same loop depth. It is possible to continue extending the region by finding
edges satisfying both these conditions, although for now the algorithm choses
to bail out.  We would like to extend this functionality in the future.

If all the previous contraints are satisfied, the algorithm returns a larger
SESE which subsumes both the SCoPs, otherwise it returns an invalid SESE to
inform that the merge was unsuccessful.

\subsection{Analysis of the SCoP detection algorithms}
The new algorithm is linear in the number of loops as it iterates on the
natural loops tree.  The number of calls to the dominators and post-dominators
is also linear in the number of loops.

The previous implementation of SCoP detection in Graphite is linear in the
number of CFG edges as it discovers regions by walking on the CFG.  In the
implementation of Polly, the use of iterated dominanace frontiers to build the
region trees may lead to quadratic behavior in some cases \cite{ramalingam}.
This can be expensive when the function body is large, specially with aggressive
inlining.

The complexity of the validity function is still a point that could be improved
in all the SCoP detection algorithms: every statement of the SESE has to be
validated.  When a SCoP is extended upwards, either including in the larger
region sequential loops, or going from an inner loop to an outer loop, the scev
instantiation point changes, and thus the scevs of inner or lower regions become
invalid under the new instantiation point, and have to be analyzed again.

The new SCoP detection algorithm helps analyze fewer statements in case of an
invalid SCoP because it focuses on the structure of the natural loops first
rather than the validity of each statement.

\section{Experimental Results}
To validate our changes to the Graphite framework, replacing the old SCoP
detection algorithm with the new one, we evaluate the number of SCoPs and number
of loops per SCoP discovered on the Polybench \cite{polybench}.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Metric                   & New  & Old  & Polly  \\
      \hline
      Number of SCoPs          & 34   & 189  & 30     \\
      Max loops/SCoP           & 17   & 8    & 11     \\
      Min loops/SCoP           & 2    & 1    & 2      \\
      Number of SCoPs with min & 7    & 109  & 3      \\
      Number of loops in SCoPs & 207  & 316  & 155    \\
      Loops/SCoP               & 6.09 & 2.59 & 5.17   \\
      \hline
    \end{tabular}
  \end{center}
  \caption{SCoP metrics.}
  \label{tab:table1}
\end{table}

Observations:
\begin{enumerate}
  \item No regressions new/original SCoP detection, the new algorithm discovers
    larger SCoPs (316 - 207 = 109).
  \item Removing limit-SCoPs discovers larger SCoPs i.e., from 2.59 to 6.09
    loops per SCoP.
\end{enumerate}

\section{Conclusion and Future Work}
We have shown that our new algorithm of SCoP detection is faster in terms of
time complexity and also detects larger SCoPs w.r.t. both previous Graphite and
LLVM-Polly implementations. This stems from the fact that operating on a higher
level data structure allows us to quickly discard uninteresting regions. We
could not get experimental data on the compile-time improvements during the SCoP
detection process because it was very difficult to precisely profile only the
SCoP-detection part of the polyhedral optimization framework. The current timing
infrastructure of gcc isn't precise enough to benchmark portions of passes which
are non-dominating on the overall compile time.

Currently, while merging two SCoPs, we try to find an SESE enclosing both. There
are techniques to add basic blocks and form SESE enclosing the SCoPs if no such
SESE was found. We would like to add this functionality in future. Since this
would have further impact on compile time we would like to do this only on hot
code paths by using the profile information.

Before starting the SCoP detection process we need to canonicalize loops to
closed SSA form. While this does not add any extra operations to the program, it
is certainly not desirable to modify the original program during the analysis
phase of any optimization pass. We would try to address this in future.  Also,
we would try to find ways to experimentally verify effectiveness of our
algorithm.

\section{Acknowledgements}
We would like to thank Samsung Austin Research and Development Center for
supporting our work on improving Graphite and Polly.  We would also like to
thank Tobias Grosser, Johannes Doerfert, and Michael Kruse for discussions about
how to improve the existing SCoP detection in Polly.

\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
