\documentclass{sigplanconf}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{Impact'16}{January 18--20, 2016, Prague, Cz}
\copyrightyear{2016} 
\doi{nnnnnnn.nnnnnnn}

\title{SCoP Detection: A Fast Algorithm for Industrial Compilers}
\authorinfo{Aditya Kumar, Sebastian Pop}
\authorinfo{Samsung Austin R\&D Center}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Loop optimizations are usually described on languages like Fortran where loops
and arrays are well behaved syntactic constructs.  In contrast with Fortran, low
level languages like C or C++ do not offer the same straight forward ease in
analysis of loops, induction variables, memory references, and data dependences,
essential to all high level loop transforms.  Compilers for low level languages
like LLVM and GCC lower the statements and expressions into low level constructs
common to all imperative programming languages: loops are represented by their
control flow, i.e., jumps between basic blocks, memory references are lowered
into pointer accesses, and expressions are lowered into three address code.
Thus, high level C++ expressions containing exceptions are represented on a low
level representation by an explicit complex control flow, that was implicit in
the high level language.

From a practical point of view, optimization passes in compilers like GCC and
LLVM are implemented on this low level representation, in order to avoid
duplicating analyses and optimizations for each supported language, and in order
to abstract away from the specifics of each language: for instance, consider the
semantics variability of loop statements in Fortran, C, C++, Java, and Ada
languages, all currently compiled and optimized by the middle-end of GCC.  The
price paid for the generality of the approach is extra compilation time spent in
recognizing all the high level loop constructs from the low level
representation.  This paper describes a fast algorithm to discover, from a low
level representation, loops that can be handled and optimized by a polyhedral
compiler.

\subsection{What are the boundaries of a SCoP?}

Regions of code that can be handled in the polyhedral model are usually called
Static Control Parts \cite{Bondhugula}, abbreviated SCoPs.
Usually, SCoPs may only contain regular
control flow void of exceptions or other constructs that may provoke changes in
control flow such as condition expressions dependent on data read from memory or
side effects of function calls.  As the compiler cannot easily handle such
constructs in the polyhedral model, these statements will not be integrated in a
SCoP, causing a split of the region containing such difficult statements into
two SCoPs.

To extend the applicability of polyhedral compilation, \cite{scopExtend}
presents techniques to represent general conditions, enlarging the limits of
SCoPs to full function body.  We will not consider these SCoP extension
techniques in the current paper, as we want a fast SCoP detection suitable to be
turned on by default in usual optimization levels, like ``-O2'', ``-O3'', or
``-Ofast''.  For that, we need an algorithm that is able to quickly converge on
the largest SCoPs that can profitably be transformed by a polyhedral compiler
like ISL \cite{verdoolaege2010isl} within a reasonable amount of compilation time.
Practically, on a large number of compiled programs, we want to use close to
zero overhead for functions without loops, or non interesting loops for the
polyhedral compilation, and less than ten percent of the overall compilation
time when optimizing loops for hot paths as shown in the profile of the compiled
program.

\subsection{High level overview of SCoP detection algorithms}

Existing implementations of SCoP detection based on low level representations
are based on some form of representation of the control flow graph: the first
implementation we did for Graphite \cite{graphite} was based on the control
flow graph itself and the basic block dominators tree.  Then Polly improved on
this SCoP detection \cite{polly} by working on an abstraction of the control flow graph, the
single entry single exit (SESE) region tree, a representation that integrates
the dominators tree on the control flow graph \cite{sese}.

In order to avoid constructing the region tree, and to help the SCoP detection
algorithm to faster converge on the parts of code that matter to polyhedral
compilation, we used an abstraction of the control flow graph, the natural loops
tree \cite{dragonbook}, together with dominance information.  In terms of
compilation time, both representations come at zero cost, as they are maintained
intact and correct between GCC's middle-end optimization passes, lowering the
compilation cost for the most numerous functions compiled by GCC: functions with
no loops, or with non contiguous isolated loops that cannot be profitably
transformed in a polyhedral compilation.

\subsection{Contributions of this paper}
\begin{enumerate}
  \item We present a new algorithm for SCoP detection, that improves over the
    existing techniques of extracting SCoPs from a low level intermediate
    representation,
  \item We provide a comparative analysis of the earlier algorithm implemented
    in Graphite, the SCoP detection as implemented in LLVM-Polly, and the new
    SCoP detection that we implemented and integrated in GCC 6.0,
  \item We provide an analysis of the shape of SCoPs detected by these three
    algorithms in terms of number of SCoPs detected in benchmarks, and number of
    loops per SCoP.
  \item Finally, we present experimental results on compile time improvements.
\end{enumerate}

\newpage
\section{Comparative analysis of SCoP detection algorithms}

We first describe the SCoP detection algorithms implemented originally in
Graphite and then in Polly, and then we show how the new algorithm improves over
these earlier implementations.  The common point of all these algorithms is that
they work on a low level representation of the program.  Analysis passes are
building more synthetic representations as described in the next subsection.

\subsection{Code analysis: from low level to higher level representations}

Starting from a low level representation of the program, a compiler extracts
information about specific aspects of the program through program analysis:

\begin{itemize}
\item The control flow graph (CFG) \cite{dragonbook} is built on top of a goto
  based intermediate representation: the nodes of the CFG represent the largest
  number of statements to be executed sequentially without branches, and the
  edges of the CFG correspond to the jumps between basic blocks.

\item The basic block dominator (DOM) and post-dominator (Post-DOM) trees
  \cite{dragonbook} represent the relation between basic blocks with respect to
  the control flow properties of the program: a basic block A is said to
  dominate another basic block B when all execution paths from the beginning of
  the function have to pass through A before reaching B.  Similarly, the
  post-dominator information is obtained by inverting the direction of all edges
  in the CFG and then asking the same question with respect to the block ending
  the function: A is said to post-dominate B when all paths from the end of the
  function have to pass through A to reach B in the edge-reversed CFG.

\item The single entry single exit (SESE) regions \cite{sese} are
  obtained from the CFG and the DOM and Post-DOM trees by identifying contiguous
  sequences of basic blocks dominated and post-dominated by unique basic blocks.
  The SESE regions may contain sub regions that have the SESE property: this
  inclusion relation is represented as a tree.

\item Natural loops \cite{dragonbook} are detected as strongly connected
  components \cite{tarjan} on the CFG, loop nesting and sequence are represented
  under the form of a tree: loop nodes are linked with $inner$ loop, and $next$
  loop relations.  The function body is represented as a loop at the root of the
  loop tree, and its depth is zero.  The depth of inner loops is one more than
  their parent, and sibling loops linked through $next$ are at the same depth.

  % Do we really need to document this?  Are you using this property in the rest of the paper?
  %The root node has in-degree of 0, all other loop nodes have in-degree of one.
  % -- This we can remove but we need to mention that an outer loop only points to one inner loop
  % -- where all inner loops are connected by $next$ relation.

  % Same questions here.  If you need to speak about reducibility, you have to define it:
  %A CFG is said to be reducible when ...

  % Every reducible control flow graph can be represented as a loop tree.
  % -- I wanted to mention this only to show that 'almost' all programs can be analyzed for
  % -- scop detection just by using loop-tree. Such that there would be no question on
  % -- coverage of programs w.r.t. the new algorithm.

\item Static Single Assignment (SSA) form \cite{cytron}, inserts extra scalar
  variables such that each definition is unique.  The assignments to scalar
  variables are either the result of expressions, or the result of phi nodes
  placed at control flow junctions in basic blocks that are target of two or
  more control flow edges.  Assignments from phi nodes represent all the
  possible assignments considering the control flow graph.

\item An abstraction of the SSA is that of a declarative language
  \cite{spop2007} in which there are no assignments or imperative language
  constructs.  The abstraction only represents the computation of scalar
  variables that are either the result of arithmetic expressions, or the result
  of two kinds of phi nodes: loop phi nodes have two arguments, one is defined
  outside the loop containing the phi node, and the other is defined inside the
  loop.  Loop close phi nodes have only one argument that is defined in an inner
  loop.

\item The analysis of scalar evolutions (scev) \cite{scev} starts from the
  abstraction of the SSA representation described above by recognizing the
  evolution function of scalar variables for loop phi nodes, loop close phi
  nodes, and derived scalar declarations.  Loop phi nodes are declared by an
  initial value and an expression containing a self reference, when the self
  reference appears in an addition expression together with a scalar value or an
  invariant expression in the current loop, the scev represents a recursive
  function with linear or affine evolution.  Loop close phi nodes are declared
  as the last value computed by an expression that may variate in a loop, the
  scev then represents a partial recursive function.  All other scalar
  declarations can be expressed as scevs derived from declarations of other
  recursive and partial recursive functions.

\item The analysis of the number of iterations \cite{scev} provides a scev that
  represents the number of times a loop is executed.  The number of iterations
  is computed as the scev of a close phi node, or last value, of a scev starting
  at zero and incremented by one at each iteration of the loop.

\item Pointer analysis detects base and access functions for all memory
  locations that are accessed in the program.

\item Alias analysis disambiguates the base pointers and identifies which
  pointers may or do not access the same memory locations.

\item Data reference analysis uses the results of the scev analysis to determine
  the memory access patterns of pointers in loops.

\item The delinearization analysis \cite{delinearization1, delinearization2}
  reconstructs a high level representation of arrays under the form of Fortran
  subscripts.  The delinearization uses all the access functions of all the data
  references in a loop, a region, or a function, in order to make sure all
  memory accesses follow the exact same pattern, i.e., the same subscript
  dimensions are valid in all accesses.
\end{itemize}

As we have seen, gradually, these analyzes extract from low level constructs
higher level representations: later analyzes are based on earlier lower level
results, building up a castle out of basic bricks.  All this information
synthesized by the compiler allows the representation of diverse imperative
programming languages into a polyhedral form \cite{Girbal} containing a very
high level information of loop iteration domains, memory accesses, and static
and dynamic schedules.

To reduce compilation time, the SCoP detection algorithms start from the lowest
level analysis results that are commonly available and try to quickly discard
parts of code that either cannot be translated in the polyhedral model or that
cannot be profitably transformed.  The compiler has to quickly evaluate whether
a part of the code is amenable to translation in the polyhedral model before
spending time in computing higher level costly information.  For that reason,
information about the CFG structure, and the number of loops per function are
the first checks in the SCoP detection algorithms, followed by a linear walk
over all the statements of the region of code to ensure the absence of side
effects in statements, and to gather more costly information about the number of
iterations, the linearity of memory access functions and condition expressions.

The next subsection describes in more detail how all these analyzes are used in
the current implementations of the SCoP detection.

\subsection{Former algorithms for SCoP detection in Graphite and Polly}

In the process of developing Graphite, the SCoP detection was the first
algorithm that we implemented, followed by the translation to the polyhedral
model, the code generation, originally using CLooG \cite{cloog}, and finally
loop transforms.

During the implementation of the translation to and code generation from the
polyhedral model, we decided to expose scalar dependences in the dependence
graph as array dependences by translating scalars to arrays with a single
element.  This was the easiest way to have a working implementation of a full
compiler going from the SSA representation into the polyhedral representation,
and then getting out of the polyhedral representation through a code generation
that was generating back an imperative program representation not aware of the
constraints due to the declarative SSA representation: for instance, the CLooG
or ISL code generation could duplicate statements, resulting in multiple copies
of the same definitions of a unique variable, a property that does not verify
the constraints of the SSA representation.  For this reason, both Graphite and
Polly decided to translate scalar variables crossing basic blocks in the SCoP or
outside the SCoP boundaries out of SSA.  The translation of scalar variables out
of SSA has been later removed in incremental developments in Graphite and Polly,
and it constitues a serious limitation, for industrial use, in the quality of
code generated by the early implementations of these polyhedral compilers.

The first Graphite SCoP detection algorithm was implemented on a very low level
representation of CFG and DOM \cite{trifunovic}.  These representations were
too restrictive for
the scev analysis to be able to determine the loops to be considered as variant
and the loops that have to be considered as invariant, in which the scev
analysis should not analyze and instantiate further scalar variables in order to
consider them as parameters.  This resulted in a very restrictive limitation of
SCoPs that had to have one full loop fully contained in the SCoP: for instance a
sequence of two loops with no surrounding loop would not be represented as a
SCoP and the SCoP detection would split these two loops into two distinct SCoPs.
This limitation has been removed in GCC 6.0 by the improvements to the scop
detection described later in this paper.

\subsection{Polly scop detection: a region based SCoP detection}

\subsection{Maximal SCoPs and why they might not be very useful}
\label{subsec:maximality}
A Maximal SCoP is the larges such SCoP which cannot be extended further. This
concept is used in LLVM
%\cite{Tobi}
and in previous graphite implementation.
%\cite{Graphite}. 
Since the main focus of polyhedral optimizations is the loops and code
surrounding it, finding a maximal SCoP might introduce branches which are not
related to loops at all. e.g.,

%\begin{comment}
%rankdir=LR;
\digraph[scale=0.5]{abc}{
IfCondition -> TrueRegion
IfCondition -> FalseRegion
FalseRegion ->Loop1
Loop1 -> EndProgram
TrueRegion -> EndProgram
FalseRegion ->EndProgram
}
%\end{comment}

In this example The `IfCondition' and `TrueRegion' just add extra code to the
polyhedral analysis and transformations if there are no loops in it. Having
just the `Loop1' as SCoP would prove more beneficial in terms of optimizations
and overall compile time. Also, regions with no loops are not very interesting
from a production compiler's point of view where compile time is at a premium.

\section{A new faster scop detection to select relevant loop nests}
%The scope of the polyhedral program analysis and manipulation is a sequence of loop
%nests with constant strides and affine bounds. It includes non-perfectly nested loops
%and conditionals with boolean expressions of affine inequalities.
%The maximal Single-Entry Single-Exit (SESE) region of the Control Flow Graph
%(CFG) that satisfies those constraints is called a Static Control Part (SCoP). \cite{Girbal, Bondhugula, trifunovic}

%Current algorithm for SCoP detection in graphite was based on dominator tree where a tree (CFG) traversal is required
%for analyzing an SESE. The tree traversal is linear in the number of basic blocks and SCoP detection is linear in
%number of instructions. This is reasonably fast but it utilizes a generic infrastructure of SESE. With regards to
%polyhedral optimization we are only interested in subtrees with loops. So it makes more sense to utilize higher level
%semantics of CFG e.g., loop tree. Since higher level abstractions contain more statments, discarding them early
%makes algorithm converge faster.

The new algorithm iterates on the loop tree nodes. The traversal starts at a
loop-nest at depth one.  The code inside the loop and all its nested loops are
analyzed recursively for validity \ref{subsec:validity}. Once all the validity
constraints are satisfied, the loop-nest becomes a valid scop. Once a valid loop
is found the algorithm analyzes the `next' loop (loop at same depth and
immediate sibling of the loop just analyzed). If an adjacent loop is found to be
a valid SCoP as well then we try to merge both loop nests by analyzing the
combined region for validity. If the combined region represent a valid SCoP then
the combined SESE \ref{subsec:merge-sese} is saved and further analysis is
continued to extend the SCoP again. If the combined SESE is not a valid SCoP
then the first SCoP is saved and analsis starts by trying to extend the second
SCoP.

With the new approach it is faster to discard invalid loop-nests very early. The
algorithm analyzes statements which matter most by starting the scop detection
from a loop-tree node (CFG node which begins from a loop header). This allows
discarding unrepresentable loops early in the scop-detection process, thereby
discarding SESe surrounding them.  This way, the number of instructions to be
analyzed for validity reduces to a minimal set.  We start by analyzing those
statements which are inside a loop, because validity of those statements is
necessary for the validity of loop. The statements outside the loop nest can be
just excluded from the SESE if they are not valid.

Since all (?)  the optimizations enabled in polyhedral compiler ISL work on loop
nests (e.g., blocking, interchange, tiling etc.),  or on sibling loops (e.g.,
fusion), we also discard functions with less than two loops.

\subsection{Preconditions}
The alogrithm assumes following things to be true.
\begin{enumerate}
\item The loop tree represents a reducible CFG.
\item All the loops are in closed SSA form.
\item Alias information is available.
\item Dominance information is available.
\item Mechanism to compute scalar evolution of a variable, in a region, is
  available.
\end{enumerate}

\subsection{Postconditions}
The set of scops thus collected have the following properties
\begin{enumerate}
\item No SCoP intersects with another in the set.
\item Alias analysis is preserved.
\item Domincance information is preserved.
\end{enumerate}

Since this algorithm starts from the loop header, it would exclude statements
before the first and after the last loop in an SESE. Also,regions without loops
maybe excluded if they are not surrounded by loops. In that sense, SCoP detected
may not be maximal \ref{subsec:maximality}.


\subsection{Validity checks for a SCoP}
\label{subsec:validity}
An SESE region is regarded as a valid scop when it satisfies the following conditions:
\begin{enumerate}
  \item The entry basic block should have only one predecessor and the exit
    basic block should have only one successsor (i.e. an SESE).
  \item The entry should dominate the exit.
  \item The exit should post-dominate the entry.
  \item The scalar evolution of all the operands in the new region should be affine.
  \item All the statements inside the region should be reprepresentable in the
    polyhedral mode.  Only labels, pure function calls, assignments and
    comparison operations on integers are allowed.
  \item The SCoP should have at least one data reference and no more than
    'MAX\_DATA\_REFERENCE' (a parameter which can be set by the user).
  \item We also limit the number of parameters to a maximum value (set by
    programmer).
\end{enumerate}



\subsection{Merging sub-SCoPs}
We compose larger SCoPs by merging smaller ones at the same loop depth. Even if
we have analyzed the statements in a sub-scop we need to re-analyze them
because the scalar evolution of the data references change with the region of the
program under analysis. In order to merge two SESEs to form a new SESE:
\label{subsec:merge-sese}
\begin{enumerate}
\item Find a common dominator with a single entry.
\item Find a common post-dominator with a single exit.
\end{enumerate}

\subsection{complexity of algorithm}
The algorithm is linear in number of loops which is much faster than the
previous algorithm. The algorithm is based on a very simple structural property
of loop tree.

\section{Experimental Results}
On Polybench:
\subsection{graphite original scop detection}
189 scops, max loops/scop 8, min loops/scop 1: 109 scops with 1 loop/scop, 316
loops in scops, 1.67 loops/scop without 1loop/scop, 80 scops contain 207 loops
(2.59 loops/scop)

\subsection{the new scop detection}
34 scops, max loops/scop 17, min loops/scop 2: 7 scops with 2 loops/scop, 207
loops in scops, 6.09 loops/scop

Observations:
\begin{enumerate}
  \item no regressions new/original scop detection, we only discover larger
    scops (316 - 207 = 109)
  \item removing limit-scops discovers larger scops i.e., from 2.59 to 6.09
\end{enumerate}


\subsection{Polly}
Number of scops detected: 30, max loops/scop 11, min loops/scop 2: 3 scops with
2 loops/scop, 155 loops in scops, 5.17 loops/scop

compile time:
- graphite old, new
- polly: region pass, scop detection

number of scops detected per benchmark

\section{Conclusion and Future Work}


\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
