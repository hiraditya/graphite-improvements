\documentclass{sigplanconf}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{Impact'16}{January 18--20, 2016, Prague, Cz}
\copyrightyear{2016} 
\doi{nnnnnnn.nnnnnnn}

\title{A Fast and Practical SCoP Detection Algorithm}
\subtitle{Polyhedral Compilation of High Level Loops from a Low Level Compiler Representation}
\authorinfo{Aditya Kumar, Sebastian Pop}
\authorinfo{Samsung Austin R\&D Center}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

Loop optimizations are usually described on languages like Fortran where loops
and arrays are well behaved syntactic constructs.  In contrast with Fortran, low
level languages like C or C++ do not offer the same straight forward ease in
analysis of loops, induction variables, memory references, and data dependences,
essential to all high level loop transforms.  Compilers for low level languages
like LLVM and GCC lower the statements and expressions into low level constructs
common to all imperative programming languages: loops are represented by their
control flow, i.e., jumps between basic blocks, memory references are lowered
into pointer accesses, and expressions are lowered into three address code.
Thus, high level C++ expressions containing exceptions are represented on a low
level representation by an explicit complex control flow, that was implicit in
the high level language.

From a practical point of view, optimization passes in compilers like GCC and
LLVM are implemented on this low level representation, in order to avoid
duplicating analyses and optimizations for each supported language, and in order
to abstract away from the specifics of each language: for instance, consider the
semantics variability of loop statements in Fortran, C, C++, Java, and Ada
languages, all currently compiled and optimized by the middle-end of GCC.  The
price paid for the generality of the approach is extra compilation time spent in
recognizing all the high level loop constructs from the low level
representation.  This paper describes a fast algorithm to discover, from a low
level representation, loops that can be handled and optimized by a polyhedral
compiler.

\subsection{What are the boundaries of a SCoP?}

Regions of code that can be handled in the polyhedral model are usually called
Static Control Parts abbreviated SCoPs.  Usually, SCoPs may only contain regular
control flow void of exceptions or other constructs that may provoke changes in
control flow such as condition expressions dependent on data read from memory or
side effects of function calls.  As a compiler cannot easily handle such
constructs in the polyhedral model, these statements will not be integrated in a
SCoP, causing a split of the region containing such difficult statements into
two SCoPs.

To extend the applicability of polyhedral compilation, \cite{scopExtend}
presents techniques to represent general conditions, enlarging the limits of
SCoPs to full function body.  We will not consider these SCoP extension
techniques in the current paper, as we want a fast SCoP detection suitable to be
turned on by default in usual optimization levels, like ``-O2'', ``-O3'', or
``-Ofast''.  For that, we need an algorithm that is able to quickly converge on
the largest SCoPs that can profitably be transformed by a polyhedral compiler
like ISL \cite{verdoolaege2010isl} within a reasonable amount of compilation time.
Practically, on a large number of compiled programs, we want to use close to
zero overhead for functions without loops, or non interesting loops for the
polyhedral compilation, and less than ten percent of the overall compilation
time when optimizing loops for hot paths as shown in the profile of the compiled
program.

\subsection{High level overview of SCoP detection algorithms}

Existing implementations of SCoP detection based on low level representations
are based on some form of representation of the control flow graph: the first
implementation we did for Graphite \cite{graphite} was based on the control
flow graph itself and the basic block dominators tree.  Then Polly improved on
this SCoP detection \cite{polly} by working on an abstraction of the control flow graph, the
single entry single exit (SESE) region tree, a representation that integrates
the dominators tree on the control flow graph \cite{sese}.

In order to avoid constructing the region tree, and to help the SCoP detection
algorithm to faster converge on the parts of code that matter to polyhedral
compilation, we used an abstraction of the control flow graph, the natural loops
tree \cite{dragonbook}, together with dominance information.  In terms of
compilation time, both representations come at zero cost, as they are maintained
intact and correct between GCC's middle-end optimization passes, lowering the
compilation cost for the most numerous functions compiled by GCC: functions with
no loops, or with non contiguous isolated loops that cannot be profitably
transformed in a polyhedral compilation.

\subsection{Contributions of this paper}
\begin{enumerate}
  \item We present a new algorithm for SCoP detection, that improves over the
    existing techniques of extracting SCoPs from a low level intermediate
    representation,
  \item we provide a comparative analysis of the earlier algorithm implemented
    in Graphite, the SCoP detection as implemented in LLVM-Polly, and the new
    SCoP detection that we implemented and integrated in GCC 6.0,
  \item we provide an analysis of the shape of SCoPs detected by these three
    algorithms in terms of number of SCoPs detected in benchmarks, and number of
    loops per SCoP.
  \item finally, we present experimental results on compile time improvements.
\end{enumerate}

\section{Comparative analysis of SCoP detection algorithms}

We first describe the SCoP detection algorithms implemented originally in
Graphite and then in Polly, and then we show how the new algorithm improves over
these earlier implementations.  The common point of all these algorithms is that
they work on a low level representation of the program.  Analysis passes are
building more synthetic representations as described in the next subsection.

\subsection{Code analysis: from low level to higher level representations}

Starting from a low level representation of the program, a compiler extracts
information about specific aspects of the program through program analysis:

\begin{itemize}
\item The control flow graph (CFG) \cite{dragonbook} is built on top of a goto
  based intermediate representation: the nodes of the CFG represent the largest
  number of statements to be executed sequentially without branches, and the
  edges of the CFG correspond to the jumps between basic blocks.

\item The basic block dominator (DOM) and post-dominator (Post-DOM) trees
  \cite{dragonbook} represent the relation between basic blocks with respect to
  the control flow properties of the program: a basic block A is said to
  dominate another basic block B when all execution paths from the beginning of
  the function have to pass through A before reaching B.  Similarly, the
  post-dominator information is obtained by inverting the direction of all edges
  in the CFG and then asking the same question with respect to the block ending
  the function: A is said to post-dominate B when all paths from the end of the
  function have to pass through A to reach B in the edge-reversed CFG.

\item The single entry single exit (SESE) regions \cite{sese} are
  obtained from the CFG and the DOM and Post-DOM trees by identifying contiguous
  sequences of basic blocks dominated and post-dominated by unique basic blocks.
  The SESE regions may contain sub regions that have the SESE property: this
  inclusion relation is represented as a tree.

\item Natural loops \cite{dragonbook} are detected as strongly connected
  components \cite{tarjan} on the CFG, loop nesting and sequence are represented
  under the form of a tree: loop nodes are linked with inner loop, and next loop
  relations.

\item Static Single Assignment (SSA) form \cite{cytron}, inserts extra scalar
  variables such that each definition is unique.  The assignments to scalar
  variables are either the result of expressions, or the result of phi nodes
  placed at control flow junctions in basic blocks that are target of two or
  more control flow edges.  Assignments from phi nodes represent all the
  possible assignments considering the control flow graph.

\item An abstraction of the SSA is that of a declarative language
  \cite{spop2007} in which there are no assignments or imperative language
  constructs.  The abstraction only represents the computation of scalar
  variables that are either the result of arithmetic expressions, or the result
  of two kinds of phi nodes: loop phi nodes have two arguments, one is defined
  outside the loop containing the phi node, and the other is defined inside the
  loop.  Loop close phi nodes have only one argument that is defined in an inner
  loop.

\item The analysis of scalar evolutions (scev) \cite{scev} starts from the
  abstraction of the SSA representation described above by recognizing the
  evolution function of scalar variables for loop phi nodes, loop close phi
  nodes, and derived scalar declarations.  Loop phi nodes are declared by an
  initial value and an expression containing a self reference, when the self
  reference appears in an addition expression together with a scalar value or an
  invariant expression in the current loop, the scev represents a recursive
  function with linear or affine evolution.  Loop close phi nodes are declared
  as the last value computed by an expression that may variate in a loop, the
  scev then represents a partial recursive function.  All other scalar
  declarations can be expressed as scevs derived from declarations of other
  recursive and partial recursive functions.

\item The analysis of the number of iterations \cite{scev} provides a scev that
  represents the number of times a loop is executed.  The number of iterations
  is computed as the scev of a close phi node, or last value, of a scev starting
  at zero and incremented by one at each iteration of the loop.

\item Pointer analysis detects base and access functions for all memory
  locations that are accessed in the program.

\item Alias analysis disambiguates the base pointers and identifies which
  pointers may or do not access the same memory locations.

\item Data reference analysis uses the results of the scev analysis to determine
  the memory access patterns of pointers in loops.

\item The delinearization analysis \cite{delinearization1, delinearization2}
  reconstructs a high level representation of arrays under the form of Fortran
  subscripts.  The delinearization uses all the access functions of all the data
  references in a loop, a region, or a function, in order to make sure all
  memory accesses follow the exact same pattern, i.e., the same subscript
  dimensions are valid in all accesses.
\end{itemize}

As we have seen, gradually, these analyzes extract from low level constructs
higher level representations: later analyzes are based on earlier lower level
results, building up a castle out of basic bricks.  All this information
synthesized by the compiler allows the representation of diverse imperative
programming languages into a polyhedral form \cite{Girbal} containing a very
high level information of loop iteration domains, memory accesses, and static
and dynamic schedules.

The next subsection describes in more detail how all these analyzes are used in
current implementations of the SCoP detection: basically the compiler has to
ensure that a part of the code is amenable to translation into the polyhedral
model before starting the translation.

\subsection{Current algorithm for scop detection in Graphite}

Describe the current status of Graphite scop detection: limitations, and compilation time.

Part of limitation, the functionality limit-scop was added as an intermediate
step to discard the loops which graphite could not handle. Removing limit-scop
required handling of different cases of loops and surrounding code.  The scop is
now larger so most test cases required 'number of scops detected' to be
fixed. By increasing the size of scop we can now optimize loops which are
'siblings' of each other. This could enable loop fusion on a number of loops.

\subsection{Polly scop detection: a region based SCoP detection}

\subsection{Maximal SCoPs and why they might not be very useful.}
A Maximal SCoP is the larges such SCoP which cannot be extended further. This concept is used
in LLVM 
%\cite{Tobi}
and in previous graphite implementation.
%\cite{Graphite}. 
Since the main focus of
polyhedral optimizations is the loops and code surrounding it, finding a maximal SCoP might introduce branches
which are not related to loops at all. e.g.,

\begin{comment}
Scop
Cond
|
|--True Region
|
|--False Region
|       | Loop1
|       |
\end{comment}

In this example The cond and True regions just add extra code to the polyhedral analysis and transformations if there
are no loops in it. Even if there are loops in True region they better be analyzed separately because
execution of Loop1 and True Region are mutually exclusive.
Having just the Loop1 as SCoP would prove more beneficial in terms of optimizations and overall compile time.



\section{A new faster scop detection to select relevant loop nests}
The scope of the polyhedral program analysis and manipulation is a sequence of loop
nests with constant strides and affine bounds. It includes non-perfectly nested loops
and conditionals with boolean expressions of affine inequalities.
The maximal Single-Entry Single-Exit (SESE) region of the Control Flow Graph
(CFG) that satisfies those constraints is called a Static Control Part (SCoP). \cite{Girbal, Bondhugula, trifunovic}

Current algorithm for SCoP detection in graphite was based on dominator tree where a tree (CFG) traversal is required
for analyzing an SESE. The tree traversal is linear in the number of basic blocks and SCoP detection is linear in
number of instructions. This is reasonably fast but it utilizes a generic infrastructure of SESE. With regards to
polyhedral optimization we are only interested in subtrees with loops. So it makes more sense to utilize higher level
semantics of CFG e.g., loop tree. Since higher level abstractions contain more statments, discarding them early
makes algorithm converge faster.

The new algorithm is geared towards tree traversal on loop structure. The algorithm is linear in number of loops
which is much faster than the previous algorithm. The algorithm is based on a very simple structural property of loop
tree.

LoopTree = LoopNest
LoopNest = LoopTree; nested loop

Briefly, we start the traversal at a loop-nest and analyze it recursively for validity. Once a valid loop is
found we find a valid adjacent loop. If an adjacent loop is found then we merge both loop nests
other wise we form a SCoP and resume the algorithm from the adjacent loop nest. The data structure to represent an SESE
is an ordered pair of edges (entry, exit). Choosing a simple data structure allows us to extend a SCoP in both the
directions. With this approach, the number of instructions to be analyzed for validity reduces to a minimal set.
We start by analyzing those statements which are inside a loop, because validity of those statements is
necessary for the validity of loop. The statements outside the loop nest can be just excluded from the
SESE if they are not valid.

In the graphite framework we do not include statements outside before the first and after the last loop in an SESE.
So in that sense, SCoP detected by this function may not be maximal.

GIMPLE statements belonging to the SCoP should not contain calls to functions with
side effects (pure and const function calls are allowed) and the only memory references
that are allowed are accesses through arrays with affine subscript functions.

To make the scop detection simpler we canonicalize the loops into a closed SSA form.
The previous graphite framework detected scop by analyzing an SESE. That approach resulted in a highly recursive structure
with redundancies at several places and hence, increase in compile time. In the new framework, we try to discard irrelevant
regions as fast as we can. Essentially,
\begin{enumerate}
\item Discard functions with less than two loops: Since we are mostly intersted in optimizing loop nests
or loop which has at least one sibling.
\item Start the scop detection from a CFG node which begins from a loop header.
\item Break the SCoP at a point where we find a statement which cannot be represented by graphite.
\item 
\end{enumerate}


\subsection{merging sub-scops to form larger scops}
We compose larger SCoPs by merging smaller ones at the same loop depth. After combining the sub-scops
the newly formed scop has to be reanalyzed for all the validity requirements:
\begin{enumerate}
  \item The entry basic block should have only one predecessor and the exit basic block should have only one
    successsor.
  \item The entry should dominate the exit.
  \item The exit should post-dominate the entry.
  \item Writes to outside of SCoP.
  \item The scalar evolution of all the operands in the new region should be affine.

    \begin{enumerate}
      \item The statement should not have any side effects.
      \item Only label, pure call, assignments and comparison operations on integers are allowed.
    \end{enumerate}
\end{enumerate}

Even if we have analyzed the statements in a sub-scop we need to redo the whole thing because the scalar evolution
of the references change with the scope of the program under analysis.


\section{Experimental Results}

On Polybench:
on graphite original scop detection, 189 scops, max loops/scop 8, min loops/scop 1: 109 scops with 1 loop/scop, 316 loops in scops, 1.67 loops/scop
without 1loop/scop, 80 scops contain 207 loops -> 2.59 loops/scop.

on the new scop detection: 34 scops, max loops/scop 17, min loops/scop 2: 7 scops with 2 loops/scop, 207 loops in scops, 6.09 loops/scop

=> no regressions new/original scop detection, we only discover larger scops (316 - 207 = 109)
=> removing limit-scops discovers larger scops: 2.59 -> 6.09

on polly: Number of scops detected: 30, max loops/scop 11, min loops/scop 2: 3 scops with 2 loops/scop, 155 loops in scops, 5.17 loops/scop


compile time:
- graphite old, new
- polly: region pass, scop detection

number of scops detected per benchmark


\section{Conclusion and Future Work}


\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
